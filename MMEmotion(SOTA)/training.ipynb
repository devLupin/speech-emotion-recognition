{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a48df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_RAVDESS_DS(path_audios):\n",
    "    \"\"\"\n",
    "    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:\n",
    "     ______________________________________________________________________________________________________________________________\n",
    "    |             name            |                     path                                   |     emotion      |     actor     |\n",
    "    ______________________________________________________________________________________________________________________________\n",
    "    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |\n",
    "    ______________________________________________________________________________________________________________________________\n",
    "    ...\n",
    "    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)\n",
    "    \"\"\"\n",
    "    dict_emotions_ravdess = {\n",
    "        0: 'Neutral',\n",
    "        1: 'Calm',\n",
    "        2: 'Happy',\n",
    "        3: 'Sad',\n",
    "        4: 'Angry',\n",
    "        5: 'Fear',\n",
    "        6: 'Disgust',\n",
    "        7: 'Surprise'\n",
    "    }\n",
    "    data = []\n",
    "    for path in tqdm(Path(path_audios).glob(\"**/*.wav\")):\n",
    "        name = str(path).split('/')[-1].split('.')[0]\n",
    "        label = dict_emotions_ravdess[int(name.split(\"-\")[2]) - 1]  # Start emotions in 0\n",
    "        actor = int(name.split(\"-\")[-1])\n",
    "\n",
    "        try:\n",
    "            data.append({\n",
    "                \"name\": name,\n",
    "                \"path\": path,\n",
    "                \"emotion\": label,\n",
    "                \"actor\": actor\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # print(str(path), e)\n",
    "            pass\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_test(fold, df, save_path=\"\"):\n",
    "    \"\"\"\n",
    "    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training\n",
    "    of each fold.\n",
    "    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]\n",
    "    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function\n",
    "    :param save_path:[str] Path to save the train.csv and test.csv per fold\n",
    "    \"\"\"\n",
    "    actors_per_fold = {\n",
    "        0: [2,5,14,15,16],\n",
    "        1: [3, 6, 7, 13, 18],\n",
    "        2: [10, 11, 12, 19, 20],\n",
    "        3: [8, 17, 21, 23, 24],\n",
    "        4: [1, 4, 9, 22],\n",
    "    }\n",
    "\n",
    "    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]\n",
    "    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    if(save_path!=\"\"):\n",
    "        train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "        test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45869f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_libs(seed=2020):\n",
    "    \"\"\"\n",
    "       Fix the seeds for the random generators of torch and other libraries\n",
    "       :param seed: Seed to pass to the random seed generators\n",
    "       \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def seed_torch(seed=2020):\n",
    "    \"\"\"\n",
    "    Fix the seeds for the random generators of torch and other libraries\n",
    "    :param seed: Seed to pass to the random seed generators\n",
    "    \"\"\"\n",
    "\n",
    "    seed_libs(2020)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced04a5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING DATA IN:  FineTuningWav2Vec2_out\\data\\20220925_201018\\fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2880it [00:00, 93147.10it/s]\n",
      "Using custom data configuration default-ba4989c74941ac51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\devLupin\\.cache\\huggingface\\datasets\\csv\\default-ba4989c74941ac51\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020943164825439453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac7bb35c0404d0e86e8422e5fe3f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0169522762298584,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17b6e4554af4edeb720aad7c50e3d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\devLupin\\.cache\\huggingface\\datasets\\csv\\default-ba4989c74941ac51\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01495981216430664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c22edfac184254b36d8e27c50a39c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold:  0  - actors in Train fold:  {1, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24}\n",
      "Processing fold:  0  - actors in Eval fold:  {2, 5, 14, 15, 16}\n",
      "A classification problem with 8 classes: ['Angry', 'Calm', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "The target sampling rate: 16000\n",
      "Generating training...\n",
      "Generating test...\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running training *****\n",
      "  Num examples = 2280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 2850\n",
      "C:\\Users\\devLupin\\miniconda3\\envs\\MME\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='2850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/2850 06:25 < 6:06:49, 0.13 it/s, Epoch 0.18/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.091200</td>\n",
       "      <td>2.168486</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.112400</td>\n",
       "      <td>2.081263</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.098400</td>\n",
       "      <td>2.111488</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.106800</td>\n",
       "      <td>2.081519</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.026300</td>\n",
       "      <td>2.103520</td>\n",
       "      <td>0.141667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-10\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-10\\config.json\n",
      "Model weights saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-10\\pytorch_model.bin\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-10\\preprocessor_config.json\n",
      "C:\\Users\\devLupin\\miniconda3\\envs\\MME\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-20\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-20\\config.json\n",
      "Model weights saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-20\\pytorch_model.bin\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-20\\preprocessor_config.json\n",
      "C:\\Users\\devLupin\\miniconda3\\envs\\MME\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-30\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-30\\config.json\n",
      "Model weights saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-30\\pytorch_model.bin\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-30\\preprocessor_config.json\n",
      "C:\\Users\\devLupin\\miniconda3\\envs\\MME\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-40\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-40\\config.json\n",
      "Model weights saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-40\\pytorch_model.bin\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-40\\preprocessor_config.json\n",
      "C:\\Users\\devLupin\\miniconda3\\envs\\MME\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Wav2Vec2ForSpeechClassification.forward` and have been ignored: actor, emotion, path, name.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 600\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-50\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-50\\config.json\n",
      "Model weights saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-50\\pytorch_model.bin\n",
      "Configuration saved in FineTuningWav2Vec2_out\\trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\\20220925_201018\\fold0\\checkpoint-50\\preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['LC_ALL'] ='C.UTF-8'\n",
    "os.environ['LANG'] = 'C.UTF-8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"-1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig\n",
    "from transformers import EvalPrediction\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from DataCollatorCTCWithPadding import *\n",
    "from Wav2VecAuxClasses import *\n",
    "from CTCTrainer import *\n",
    "from my_functions import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "#Read input parameters\n",
    "audios_dir = 'audio_16k'\n",
    "cache_dir = 'MMEmotionRecognition/data/Audio/cache_dir'\n",
    "out_dir = 'FineTuningWav2Vec2_out'\n",
    "model_id = 'jonatasgrosman/wav2vec2-large-xlsr-53-english'\n",
    "\n",
    "#PARAMETERS #######################\n",
    "out_dir_models = os.path.join(out_dir, \"trained_models/wav2vec2-xlsr-ravdess-speech-emotion-recognition\") #out path to save trained models\n",
    "data_path = os.path.join(out_dir,\"data\") #Path to save csvs generated containing the recording information (path, name, emotion...)\n",
    "\n",
    "# We need to specify the input and output column\n",
    "input_column = \"path\" # Name of the column that will contain the path of the recordings\n",
    "output_column = \"emotion\" # Name of the column that will contain the labels of the recordings\n",
    "pooling_mode = \"mean\" #Type of pooling to apply to the embeddings generated ath the output of the transformer module to collapse all the timesteps of the recordingsinto a single vector\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "seed = 2020\n",
    "epochs = 10 #Epochs to train the model\n",
    "\n",
    "#PARAMETERS #######################\n",
    "seed_torch(seed=seed) #Set random seeds\n",
    "\n",
    "for fold in range(5): # 5-CV strategy\n",
    "    #Define paths, create aux. folders and callbacks to save data\n",
    "    out_dir_models_path = os.path.join(out_dir_models, current_time, \"fold\"+str(fold))\n",
    "    save_path = os.path.join(data_path, current_time, \"fold\"+str(fold))\n",
    "    os.environ['TRANSFORMERS_CACHE'] = os.path.join(cache_dir, current_time, \"fold\"+str(fold))\n",
    "    os.environ['HF_DATASETS_CACHE'] = os.path.join(cache_dir, current_time, \"fold\"+str(fold))\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(\"SAVING DATA IN: \", save_path)\n",
    "    callbackTB = transformers.integrations.TensorBoardCallback()\n",
    "\n",
    "    #######################\n",
    "    #PREPARE DATASET\n",
    "    #Generate complete dataframe with RAVDESS samples\n",
    "    df = prepare_RAVDESS_DS(audios_dir)\n",
    "    _, _ = generate_train_test(fold, df, save_path)\n",
    "    time.sleep(10) #wait some time to get the dataset ready\n",
    "    data_files = {\n",
    "        \"train\": os.path.join(save_path, \"train.csv\"),\n",
    "        \"validation\": os.path.join(save_path, \"test.csv\"),\n",
    "    }\n",
    "    \n",
    "    #Load data\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    print(\"Processing fold: \", str(fold), \" - actors in Train fold: \",set(train_dataset[\"actor\"]))\n",
    "    print(\"Processing fold: \", str(fold), \" - actors in Eval fold: \", set(eval_dataset[\"actor\"]))\n",
    "    label_list = train_dataset.unique(output_column)\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"A classification problem with {num_labels} classes: {label_list}\")\n",
    "    \n",
    "    # LOAD PRE-TRAINED MODEL ON ASR\n",
    "    # config\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_id, #path to the model of HuggingFace lib. that we will use as baseline to fine-tune.\n",
    "        num_labels=num_labels, # num classes\n",
    "        label2id={label: i for i, label in enumerate(label_list)}, # dict that maps emotions -> numbers\n",
    "        id2label={i: label for i, label in enumerate(label_list)}, # dict that maps numbers -> emotions\n",
    "        finetuning_task=\"wav2vec2_clf\",\n",
    "    )\n",
    "    \n",
    "    #Add in the config variable the 'pooling_mode'\n",
    "    setattr(config, 'pooling_mode', pooling_mode)\n",
    "    \n",
    "    #Load the processor for the type of model (Wav2Vec2.0 in our case) and get the expected sampling rate (16kHZ in our case)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_id, )\n",
    "    target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "    print(f\"The target sampling rate: {target_sampling_rate}\")\n",
    "    print(\"Generating training...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    print(\"Generating test...\")\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    #MODEL\n",
    "    print(\"Training model...\")\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    is_regression = False\n",
    "    \n",
    "    #Create the architecture: Wav2Vec2.0 model + mean pooling + MLP (1024, 8)\n",
    "    model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "        model_id,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    #Freeze feature encoder layers (CNNs) of wav2vec2.0 & train the transformer module and the MLP that we have added (randomly initialized)\n",
    "    model.freeze_feature_extractor()\n",
    "    \n",
    "    #Set trainig arguments/parameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir_models_path,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        prediction_loss_only=False,\n",
    "        num_train_epochs=epochs,\n",
    "        fp16=True,\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        logging_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        seed=seed, )\n",
    "    \n",
    "    #Set data collator to pad the small recordings\n",
    "    trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks = [callbackTB])\n",
    "    \n",
    "    #Start training the network using the train_dataset & evaluating it on the eval_dataset passed as parameters\n",
    "    # to the CTCTrainer\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ffbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "83dfb197234db915c9a97a66c29a751c1aafa592b3eb526b1f0133022de91a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
