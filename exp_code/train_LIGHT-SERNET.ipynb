{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Lambda, Conv2D, Dropout,Dense,Activation,Input,GlobalAveragePooling1D, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Reshape,Flatten,BatchNormalization,MaxPooling1D,AveragePooling2D,Reshape,Attention, ReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from Config import Config\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'EMODB'\n",
    "CLASS_LABELS = Config.EMODB_LABELS\n",
    "k = Config.EMODB_K\n",
    "\n",
    "model_name = 'LIGHT_SERNET'\n",
    "feature_name = 'mfcc'\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SERNET(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes, L2=1e-6, DROPOUT=0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.path1 = Sequential([\n",
    "        Conv2D(32, (11, 1), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "    self.path2 = Sequential([\n",
    "        Conv2D(32, (1, 9), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "    self.path3 = Sequential([\n",
    "        Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    #### LFLB Blocks\n",
    "    self.conv1 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.relu1 = ReLU()\n",
    "    self.max_pool1 = AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "    self.top1 = Conv2D(64, (1, 1), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.down1 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    \n",
    "    self.conv2 = Conv2D(64, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.relu2 = ReLU()\n",
    "    self.max_pool2 = AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "    self.top2 = Conv2D(64, (1, 1), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.down2 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    \n",
    "    self.conv3 = Conv2D(128, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.relu3 = ReLU()\n",
    "    self.max_pool3 = AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "    self.top3 = Conv2D(64, (1, 1), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.down3 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    \n",
    "    self.conv4 = Conv2D(256, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.bn4 = BatchNormalization()\n",
    "    self.relu4 = ReLU()\n",
    "    self.max_pool4 = AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "    self.top4 = Conv2D(64, (1, 1), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.down4 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    \n",
    "    self.conv5 = Conv2D(512, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.bn5 = BatchNormalization()\n",
    "    self.relu5 = ReLU()\n",
    "    self.max_pool5 = AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "    self.top5 = Conv2D(64, (1, 1), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "    self.down5 = Conv2D(32, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "\n",
    "    self.feature = Conv2D(32*5, (3, 3), strides=1, padding='same', use_bias=False, kernel_regularizer=regularizers.l2(L2))\n",
    "\n",
    "    self.gap = GlobalAveragePooling2D()\n",
    "    self.drop = Dropout(DROPOUT)\n",
    "    self.classifier = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = inputs\n",
    "\n",
    "    path1 = self.path1(x)\n",
    "    path2 = self.path2(x)\n",
    "    path3 = self.path3(x)\n",
    "\n",
    "    x = tf.math.maximum(path1, path2)\n",
    "    x = tf.math.maximum(x, path3)\n",
    "    \n",
    "\n",
    "    p1 = self.conv1(x)\n",
    "    p1 = self.bn1(p1)\n",
    "    p1 = self.relu1(p1)\n",
    "    # p1 = self.max_pool1(p1)\n",
    "    \n",
    "    p2 = self.conv2(p1)\n",
    "    p2 = self.bn2(p2)\n",
    "    p2 = self.relu2(p2)\n",
    "    # p2 = self.max_pool2(p2)\n",
    "    \n",
    "    p3 = self.conv3(p2)\n",
    "    p3 = self.bn3(p3)\n",
    "    p3 = self.relu3(p3)\n",
    "    # p3 = self.max_pool3(p3)\n",
    "    \n",
    "    p4 = self.conv4(p3)\n",
    "    p4 = self.bn4(p4)\n",
    "    p4 = self.relu4(p4)\n",
    "    # p4 = self.max_pool4(p4)\n",
    "    \n",
    "    p5 = self.conv5(p4)\n",
    "    p5 = self.bn5(p5)\n",
    "    p5 = self.relu5(p5)\n",
    "    # p5 = self.max_pool5(p5)\n",
    "\n",
    "    top5 = self.top5(p5)\n",
    "    \n",
    "    top4 = tf.math.maximum(self.top4(p4), top5)\n",
    "    top3 = tf.math.maximum(self.top3(p3), top4)\n",
    "    top2 = tf.math.maximum(self.top2(p2), top3)\n",
    "    top1 = tf.math.maximum(self.top1(p1), top2)\n",
    "    \n",
    "    down1 = self.down1(top1)\n",
    "    down2 = self.down2(top2)\n",
    "    down3 = self.down3(top3)\n",
    "    down4 = self.down4(top4)\n",
    "    down5 = self.down5(top5)\n",
    "\n",
    "    x = Concatenate(axis=1)([down1, down2, down3, down4, down5])\n",
    "    x1 = tf.math.maximum(down1, down2)\n",
    "    x2 = tf.math.maximum(down3, down4)\n",
    "    x = tf.math.maximum(x1, x2)\n",
    "    x = tf.math.maximum(x, down5)\n",
    "    \n",
    "    x = self.feature(x)\n",
    "    \n",
    "    x = self.gap(x)\n",
    "    x = self.drop(x)\n",
    "    output = self.classifier(x)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, loss_fn, optimizer, x, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 미분 계산\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))     # 신경망 파라미터 업데이트\n",
    "    \n",
    "    acc = sum(np.squeeze(labels) == np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return loss, acc/len(labels)*100\n",
    "\n",
    "def test_step(model, loss_fn, x, labels):\n",
    "    predictions = model(x)\n",
    "    loss = loss_fn(labels, predictions)\n",
    "    acc = sum(np.squeeze(labels) == np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return loss, acc/len(labels)*100, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(f'dataset/{DATA_PATH}.npy', 'rb') as f:\n",
    "    x = np.load(f)\n",
    "    y = np.load(f)\n",
    "\n",
    "# y = to_categorical(y,num_classes=len(Config.CLASS_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dataset_name, model_name, feature_name, fold, now_time):\n",
    "    save_path = os.path.join('Models', dataset_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    naming = f'{model_name}_{feature_name}_{fold}-fold_{now_time}'\n",
    "    \n",
    "    h5_path = f'{naming}.h5'\n",
    "    model.save_weights(os.path.join(save_path, h5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_DECAY_PARAMETERS = -0.15\n",
    "LEARNING_RATE_DECAY_STRATPOINT = 50\n",
    "LEARNING_RATE_DECAY_STEP = 20\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < LEARNING_RATE_DECAY_STRATPOINT:\n",
    "        return lr\n",
    "    else:\n",
    "        if epoch % LEARNING_RATE_DECAY_STEP == 0:\n",
    "            lr = lr * tf.math.exp(LEARNING_RATE_DECAY_PARAMETERS)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_list = []\n",
    "eva_matrix = []\n",
    "\n",
    "avg_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kfold = KFold(n_splits=k, shuffle=False, random_state=None)\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(x, y)), desc='Training {k}-Fold.....'):\n",
    "    now_time = datetime.now().strftime(\"%m-%d-%H%M%S\")\n",
    "    \n",
    "    x_train, y_train = x[train], y[train]\n",
    "    # y[train] = smooth_labels(y[train], 0.1)\n",
    "    \n",
    "    x_test, y_test = x[test], y[test]\n",
    "    \n",
    "    shape = x_train.shape[1:]\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = Adam(learning_rate=1e-5)\n",
    "    \n",
    "    model = SERNET(len(CLASS_LABELS))\n",
    "    \n",
    "    best_test_loss = 0x3f3f3f\n",
    "    best_test_acc = -1\n",
    "    \n",
    "    epoch_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    batch_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(2022).batch(BATCH)\n",
    "    batch_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH)\n",
    "    for epoch in tqdm(range(EPOCHS), desc=f'Fold-{i+1}'):\n",
    "\n",
    "        train_loss, train_acc = [], []\n",
    "        for features, labels in batch_train:\n",
    "            loss, acc = train_step(model, loss_fn, optimizer, features, labels)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        test_loss, test_acc = [], []\n",
    "        for features, labels in batch_test:\n",
    "            loss, acc, _ = test_step(model, loss_fn, features, labels)\n",
    "            test_loss.append(loss)\n",
    "            test_acc.append(acc)\n",
    "        \n",
    "        epoch_loss = sum(train_loss)/len(train_loss)\n",
    "        epoch_acc = sum(train_acc)/len(train_acc)\n",
    "        val_loss = sum(test_loss)/len(test_loss)\n",
    "        val_acc = sum(test_acc)/len(test_acc)\n",
    "        \n",
    "        epoch_losses.append(epoch_loss)\n",
    "        valid_losses.append(val_loss)\n",
    "        \n",
    "        cur_lr = K.eval(optimizer.lr)\n",
    "        print(f'{epoch+1}/{EPOCHS} lr={cur_lr:.5f} - loss:{epoch_loss:.3f}, acc:{epoch_acc:.3f}, val_loss:{val_loss:.3f}, val_acc:{val_acc:.3f}')\n",
    "        print(f'Best loss:{best_test_loss:.3f}, Best accuracy:{best_test_acc:.3f}')\n",
    "        \n",
    "        set_lr = scheduler(epoch, K.eval(optimizer.lr))\n",
    "        K.set_value(optimizer.learning_rate, set_lr)\n",
    "        \n",
    "        if best_test_loss > val_loss:\n",
    "            best_test_acc = val_acc\n",
    "            best_test_loss = val_loss\n",
    "            y_pred_best = model.predict(x[test])\n",
    "            \n",
    "            # model save\n",
    "            save_model(model, DATA_PATH, model_name, feature_name, i, now_time)\n",
    "            \n",
    "    print(f'[*] Done - acc:{best_test_acc:.3f}')\n",
    "    \n",
    "    plt.title('Loss Curve')\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.plot(epoch_losses[:],'b')\n",
    "    plt.plot(valid_losses[:],'r')\n",
    "    plt.legend(['Training loss','Validation loss'])\n",
    "    plt.show()\n",
    "    \n",
    "    avg_acc += best_test_acc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_groundtruth_list)\n",
    "print(predicted_emotions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Report = classification_report(emotions_groundtruth_list, predicted_emotions_list)\n",
    "\n",
    "os.makedirs(f'Results/{DATA_PATH}', exist_ok=True)\n",
    "report_path = f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold_nomalize.txt'\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "emotion_names = Config.CLASS_LABELS\n",
    "\n",
    "\n",
    "# build confusion matrix and normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(emotions_groundtruth_list, predicted_emotions_list)\n",
    "conf_matrix_norm = confusion_matrix(emotions_groundtruth_list, predicted_emotions_list,normalize='true')\n",
    "\n",
    "# make a confusion matrix with labels using a DataFrame\n",
    "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_names, columns=emotion_names)\n",
    "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_names, columns=emotion_names)\n",
    "\n",
    "# plot confusion matrices\n",
    "plt.figure(figsize=(16,6))\n",
    "sn.set(font_scale=1.8) # emotion label and title size\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 13}, fmt='g') #annot_kws is value font\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df_norm, annot=True, annot_kws={\"size\": 13}) #annot_kws is value font\n",
    "plt.savefig(f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold_confmatrix.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "naming = f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold'\n",
    "naming = f'{naming}_{avg_acc/5:.3f}.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(naming)\n",
    "for i,item in enumerate(conf_matrix_list):\n",
    "    temp = {}\n",
    "    temp[\" \"] = CLASS_LABELS\n",
    "    j = 0\n",
    "    for j,l in enumerate(item):\n",
    "        temp[CLASS_LABELS[j]]=item[j]\n",
    "    data1 = pd.DataFrame(temp)\n",
    "    data1.to_excel(writer,sheet_name=str(i), encoding='utf8')\n",
    "    df = pd.DataFrame(eva_matrix[i]).transpose()\n",
    "    df.to_excel(writer,sheet_name=str(i)+\"_evaluate\", encoding='utf8')\n",
    "writer.save()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('LIGHT-SERNET')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14 | packaged by conda-forge | (default, Nov 21 2022, 13:14:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5069a7505627bbd03cc90b99af22c43f4aff4d8e01211d9e29e858623d083b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
