{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e78e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LC_ALL'] ='C.UTF-8'\n",
    "os.environ['LANG'] = 'C.UTF-8'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "355411d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_RAVDESS_DS(path_audios):\n",
    "    \"\"\"\n",
    "    Generation of the dataframe with the information of the dataset. The dataframe has the following structure:\n",
    "     ______________________________________________________________________________________________________________________________\n",
    "    |             name            |                     path                                   |     emotion      |     actor     |\n",
    "    ______________________________________________________________________________________________________________________________\n",
    "    |  01-01-01-01-01-01-01.wav   |    <RAVDESS_dir>/audios_16kHz/01-01-01-01-01-01-01.wav     |     Neutral      |     1         |\n",
    "    ______________________________________________________________________________________________________________________________\n",
    "    ...\n",
    "    :param path_audios: Path to the folder that contains all the audios in .wav format, 16kHz and single-channel(mono)\n",
    "    \"\"\"\n",
    "    dict_emotions_ravdess = {\n",
    "        0: 'Neutral',\n",
    "        1: 'Calm',\n",
    "        2: 'Happy',\n",
    "        3: 'Sad',\n",
    "        4: 'Angry',\n",
    "        5: 'Fear',\n",
    "        6: 'Disgust',\n",
    "        7: 'Surprise'\n",
    "    }\n",
    "    data = []\n",
    "    for path in tqdm(Path(path_audios).glob(\"*/*.wav\")):\n",
    "        name = str(path).split('/')[-1].split('.')[0]\n",
    "        label = dict_emotions_ravdess[int(name.split(\"-\")[2]) - 1]  # Start emotions in 0\n",
    "        actor = int(name.split(\"-\")[-1])\n",
    "\n",
    "        try:\n",
    "            data.append({\n",
    "                \"name\": name,\n",
    "                \"path\": path,\n",
    "                \"emotion\": label,\n",
    "                \"actor\": actor\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # print(str(path), e)\n",
    "            pass\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_test(fold, df, save_path=\"\"):\n",
    "    \"\"\"\n",
    "    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training\n",
    "    of each fold.\n",
    "    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]\n",
    "    :param df:[DataFrame] Dataframe with the complete list of files generated by prepare_RAVDESS_DS(..) function\n",
    "    :param save_path:[str] Path to save the train.csv and test.csv per fold\n",
    "    \"\"\"\n",
    "    actors_per_fold = {\n",
    "        0: [2,5,14,15,16],\n",
    "        1: [3, 6, 7, 13, 18],\n",
    "        2: [10, 11, 12, 19, 20],\n",
    "        3: [8, 17, 21, 23, 24],\n",
    "        4: [1, 4, 9, 22],\n",
    "    }\n",
    "\n",
    "    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]\n",
    "    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    if(save_path!=\"\"):\n",
    "        train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "        test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7c3686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae2123261e440d88213bf27fe1022cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe729d849f1440b490301a6eed9274ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "make csv files.....:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "\n",
    "df = prepare_RAVDESS_DS('dataset')\n",
    "\n",
    "for fold in tqdm(range(5), desc='make csv files.....'):\n",
    "    save_path = os.path.join('audio_48k', \"fold\"+str(fold))\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    generate_train_test(fold, df, save_path)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0234a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "\n",
    "def speech_file_to_array_fn(path, sample_rate=48000):\n",
    "    \"\"\"\n",
    "    Loader of audio recordings. It loads the recordings and convert them to a specific sampling rate if required, and returns\n",
    "    an array with the samples of the audio.\n",
    "    :param path:[str] Path to the wav file.\n",
    "    \"\"\"\n",
    "    waveform, _ = librosa.load(path, duration=3, offset=0.5, sr=sample_rate)\n",
    "    \n",
    "    waveform_homo = np.zeros((int(sample_rate*3,)))\n",
    "    waveform_homo[:len(waveform)] = waveform\n",
    "                                      \n",
    "    return waveform_homo\n",
    "\n",
    "def label_to_id(label):\n",
    "    label_list = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fear', 'Disgust', 'Surprise']\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples, input_column = \"path\", output_column = \"emotion\"):\n",
    "    \"\"\"\n",
    "    Load the recordings with their labels.\n",
    "    :param examples:[DataFrame]  with the samples of the training or test sets.\n",
    "    :param input_column:[str]  Column that contain the paths to the recordings\n",
    "    :param output_column:[str]  Column that contain the emotion associated to each recording\n",
    "    :param target_sampling_rate:[int] Global variable with the expected sampling rate of the model\n",
    "    \"\"\"\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = [label_to_id(label) for label in examples[output_column]]\n",
    "\n",
    "    result = {\n",
    "        'input_values': speech_list,\n",
    "        'labels': target_list\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69af7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_melspectrogram(\n",
    "    waveform, \n",
    "    sample_rate,\n",
    "    fft = 1024,\n",
    "    winlen = 512,\n",
    "    window='hamming',\n",
    "    hop=256,\n",
    "    mels=128,\n",
    "    ):\n",
    "    \n",
    "    melspectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, \n",
    "        sr=sample_rate, \n",
    "        n_fft=fft, \n",
    "        win_length=winlen, \n",
    "        window=window, \n",
    "        hop_length=hop, \n",
    "        n_mels=mels, \n",
    "        fmax=sample_rate/2)\n",
    "    \n",
    "    melspectrogram = librosa.power_to_db(melspectrogram, ref=np.max)\n",
    "    \n",
    "    return melspectrogram\n",
    "\n",
    "def feature_mfcc(\n",
    "    waveform, \n",
    "    sample_rate,\n",
    "    n_mfcc = 40,\n",
    "    fft = 1024,\n",
    "    winlen = 512,\n",
    "    window='hamming',\n",
    "    mels=128\n",
    "    ):\n",
    "\n",
    "    # Compute the MFCCs for all STFT frames \n",
    "    # 40 mel filterbanks (n_mfcc) = 40 coefficients\n",
    "    mfc_coefficients=librosa.feature.mfcc(\n",
    "        y=waveform, \n",
    "        sr=sample_rate, \n",
    "        n_mfcc=n_mfcc,\n",
    "        n_fft=fft, \n",
    "        win_length=winlen, \n",
    "        window=window, \n",
    "        #hop_length=hop, \n",
    "        n_mels=mels, \n",
    "        fmax=sample_rate/2\n",
    "        ) \n",
    "\n",
    "    return mfc_coefficients\n",
    "\n",
    "def get_features(waveforms, sample_rate=48000):\n",
    "\n",
    "    ret = []\n",
    "    file_count = 0\n",
    "\n",
    "    for waveform in waveforms:\n",
    "        mfccs = feature_mfcc(waveform, sample_rate)\n",
    "        ret.append(mfccs)\n",
    "        file_count += 1\n",
    "        \n",
    "#         print('\\r'+f' Processed {file_count}/{len(waveforms)} waveforms',end='')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33906c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awgn_waveforms(waveform, multiples=2, bits=16, snr_min=15, snr_max=30):\n",
    "\n",
    "    # get length of waveform (should be 3*48k = 144k)\n",
    "    wave_len = len(waveform)\n",
    "\n",
    "    # Generate normally distributed (Gaussian) noises\n",
    "    # one for each waveform and multiple (i.e. wave_len*multiples noises)\n",
    "    noise = np.random.normal(size=(multiples, wave_len))\n",
    "\n",
    "    # Normalize waveform and noise\n",
    "    norm_constant = 2.0**(bits-1)\n",
    "    norm_wave = waveform / norm_constant\n",
    "    norm_noise = noise / norm_constant\n",
    "\n",
    "    # Compute power of waveform and power of noise\n",
    "    signal_power = np.sum(norm_wave ** 2) / wave_len\n",
    "    noise_power = np.sum(norm_noise ** 2, axis=1) / wave_len\n",
    "\n",
    "    # Choose random SNR in decibels in range [15,30]\n",
    "    snr = np.random.randint(snr_min, snr_max)\n",
    "\n",
    "    # Apply whitening transformation: make the Gaussian noise into Gaussian white noise\n",
    "    # Compute the covariance matrix used to whiten each noise\n",
    "    # actual SNR = signal/noise (power)\n",
    "    # actual noise power = 10**(-snr/10)\n",
    "    covariance = np.sqrt((signal_power / noise_power) * 10 ** (- snr / 10))\n",
    "    # Get covariance matrix with dim: (144000, 2) so we can transform 2 noises: dim (2, 144000)\n",
    "    covariance = np.ones((wave_len, multiples)) * covariance\n",
    "\n",
    "    # Since covariance and noise are arrays, * is the haddamard product\n",
    "    # Take Haddamard product of covariance and noise to generate white noise\n",
    "    multiple_augmented_waveforms = waveform + covariance.T * noise\n",
    "\n",
    "    return multiple_augmented_waveforms\n",
    "\n",
    "def augment_awgn_waveforms(waveforms, features, emotions, multiples, sample_rate):\n",
    "    # keep track of how many waveforms we've processed so we can add correct emotion label in the same order\n",
    "    emotion_count = 0\n",
    "    # keep track of how many augmented samples we've added\n",
    "    added_count = 0\n",
    "    # convert emotion array to list for more efficient appending\n",
    "    emotions = emotions.tolist()\n",
    "\n",
    "    for waveform in waveforms:\n",
    "\n",
    "        # Generate 2 augmented multiples of the dataset, i.e. 1440 native + 1440*2 noisy = 4320 samples total\n",
    "        augmented_waveforms = awgn_waveforms(waveform, multiples=multiples)\n",
    "\n",
    "        # compute spectrogram for each of 2 augmented waveforms\n",
    "        for augmented_waveform in augmented_waveforms:\n",
    "\n",
    "            # Compute MFCCs over augmented waveforms\n",
    "            augmented_mfcc = feature_mfcc(\n",
    "                augmented_waveform, sample_rate=sample_rate)\n",
    "\n",
    "            # append the augmented spectrogram to the rest of the native data\n",
    "            features.append(augmented_mfcc)\n",
    "            emotions.append(emotions[emotion_count])\n",
    "\n",
    "            # keep track of new augmented samples\n",
    "            added_count += 1\n",
    "\n",
    "            # check progress\n",
    "#             print('\\r'+f'Processed {emotion_count + 1}/{len(waveforms)} waveforms for {added_count}/{len(waveforms)*multiples} new augmented samples', end='')\n",
    "\n",
    "        # keep track of the emotion labels to append in order\n",
    "        emotion_count += 1\n",
    "\n",
    "    return features, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a673ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feature_scaling(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    N, T, F = X_train.shape\n",
    "    X_train = np.reshape(X_train, (N,-1))\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_train = np.reshape(X_train, (N,T,F))\n",
    "    \n",
    "    \n",
    "    N, T, F = X_test.shape\n",
    "    X_test = np.reshape(X_test, (N,-1))\n",
    "    X_test = scaler.transform(X_test)\n",
    "    X_test = np.reshape(X_test, (N,T,F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffa834e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74335e3993844849afbeee6531181c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fold data preprocessing.....:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b8bcc55f28dc144d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-b8bcc55f28dc144d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db0fa4188b54ba28e0549b9633fea39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b744454b2e9497b9c0af3aa151fef85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-b8bcc55f28dc144d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee36c38d85b8440bba5617ec698052ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test waveforms shape\n",
      "(1140, 144000) (1140,) (300, 144000) (300,)\n",
      "train, test awgn shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,)\n",
      "train, test feature scaling shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6481803daa7c2721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-6481803daa7c2721/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c66230eedc44b49e9e62ca20f6e017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7957c04eb274447e97cf54ad8d8e71ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-6481803daa7c2721/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc1ae11f9443778c8c65cfe713ac8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test waveforms shape\n",
      "(1140, 144000) (1140,) (300, 144000) (300,)\n",
      "train, test awgn shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,)\n",
      "train, test feature scaling shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-551db4a7f964cc4f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-551db4a7f964cc4f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d471472a0d0f42859248db662910c91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86feea07905f4515ae1c4fe6b0395b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-551db4a7f964cc4f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3898cac4bd482f9673158d65d1ab61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test waveforms shape\n",
      "(1140, 144000) (1140,) (300, 144000) (300,)\n",
      "train, test awgn shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,)\n",
      "train, test feature scaling shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10664dd408e3abb0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-10664dd408e3abb0/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e727ffe4124fad8e955d7130aa88b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc2a2afeabc4767a59aff2b26397d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-10664dd408e3abb0/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc462e7f35c4de894e0eb2a0aa25323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test waveforms shape\n",
      "(1140, 144000) (1140,) (300, 144000) (300,)\n",
      "train, test awgn shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,)\n",
      "train, test feature scaling shape\n",
      "(3420, 40, 282) (3420,) (900, 40, 282) (900,) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-325a78e528c91f5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-325a78e528c91f5d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda711b48a04c9fbc78aaabb73353ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0110ad1a459d454b9c867846cb2a1c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/devLupin/.cache/huggingface/datasets/csv/default-325a78e528c91f5d/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devLupin\\miniconda3\\envs\\ser\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:704: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c15fffe7f764072ae21494945906539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test waveforms shape\n",
      "(1200, 144000) (1200,) (240, 144000) (240,)\n",
      "train, test awgn shape\n",
      "(3600, 40, 282) (3600,) (720, 40, 282) (720,)\n",
      "train, test feature scaling shape\n",
      "(3600, 40, 282) (3600,) (720, 40, 282) (720,) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "save_dir = 'numpy_48k_1d'\n",
    "\n",
    "dataset_1d = []\n",
    "for fold in tqdm(range(5), desc=f'fold data preprocessing.....'):\n",
    "    save_path = os.path.join('audio_48k', \"fold\"+str(fold))\n",
    "    \n",
    "    data_files = {\n",
    "        \"train\": os.path.join(save_path, \"train.csv\"),\n",
    "        \"validation\": os.path.join(save_path, \"test.csv\"),\n",
    "    }\n",
    "    \n",
    "    #Load data\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    \n",
    "    train = preprocess_function(train_dataset)\n",
    "    test = preprocess_function(eval_dataset)\n",
    "    \n",
    "    X_train = np.array(train[\"input_values\"])\n",
    "    y_train = np.array(train['labels'])\n",
    "    X_test = np.array(test[\"input_values\"])\n",
    "    y_test = np.array(test['labels'])\n",
    "    \n",
    "    print(f'train, test waveforms shape')\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    \n",
    "    features_train = get_features(X_train, 48000)\n",
    "    features_test = get_features(X_test, 48000)\n",
    "    \n",
    "    features_train, y_train = augment_awgn_waveforms(X_train, features_train, y_train, 2, 48000)\n",
    "    features_test, y_test = augment_awgn_waveforms(X_test, features_test, y_test, 2, 48000)\n",
    "    X_train = np.array(features_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(features_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    print(f'train, test awgn shape')\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    \n",
    "    feature_scaling(X_train, X_test)\n",
    "    print(f'train, test feature scaling shape')\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, '\\n\\n')\n",
    "    \n",
    "    numpy_name = os.path.join(save_dir, str(fold) + '.npy')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    with open(numpy_name, 'wb') as f:\n",
    "        np.save(f, X_train)\n",
    "        np.save(f, y_train)\n",
    "        np.save(f, X_test)\n",
    "        np.save(f, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
