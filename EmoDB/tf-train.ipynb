{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Lambda, Conv2D, Dropout,Dense,Activation,Input,GlobalAveragePooling1D, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Reshape,Flatten,BatchNormalization,MaxPooling1D,AveragePooling2D,Reshape,Attention, ReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from Config import Config\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Mean, SparseCategoricalAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'EMODB'\n",
    "CLASS_LABELS = Config.EMODB_LABELS\n",
    "k = Config.EMODB_K\n",
    "\n",
    "model_name = 'LIGHT_SERNET'\n",
    "feature_name = 'mfcc'\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SERNET(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes, L2=1e-6, DROPOUT=0.3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.path1 = Sequential([\n",
    "        Conv2D(32, (11, 1), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "    self.path2 = Sequential([\n",
    "        Conv2D(32, (1, 9), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "    self.path3 = Sequential([\n",
    "        Conv2D(32, (3, 3), padding=\"same\", strides=(1, 1)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=2, padding='same')\n",
    "    ])\n",
    "\n",
    "    self.LFLBs = Sequential([\n",
    "        Conv2D(64, (3, 3), strides=1, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(L2)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "        Conv2D(96, (3, 3), strides=1, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(L2)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "        Conv2D(128, (3, 3), strides=1, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(L2)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=(2, 1), padding='same'),\n",
    "\n",
    "        Conv2D(160, (3, 3), strides=1, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(L2)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        AveragePooling2D(pool_size=(2, 1), padding='same'),\n",
    "\n",
    "        Conv2D(320, (1, 1), strides=1, padding='same', use_bias=False,\n",
    "               kernel_regularizer=regularizers.l2(L2)),\n",
    "        BatchNormalization(),\n",
    "        ReLU(),\n",
    "        GlobalAveragePooling2D(),\n",
    "    ])\n",
    "\n",
    "    self.drop = Dropout(DROPOUT)\n",
    "    self.classifier = Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = inputs\n",
    "\n",
    "    path1 = self.path1(x)\n",
    "    path2 = self.path2(x)\n",
    "    path3 = self.path3(x)\n",
    "\n",
    "    x = Concatenate(axis=1)([path1, path2, path3])\n",
    "\n",
    "    x = self.LFLBs(x)\n",
    "\n",
    "    x = self.drop(x)\n",
    "    output = self.classifier(x)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(x_train, y_train, epochs=10, validation_split=0.25)\n",
    "\n",
    "def train_step(model, loss_fn, optimizer, mean_train_loss, train_accuracy, x, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # 미분 계산\n",
    "        predictions = model(x)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))     # 신경망 파라미터 업데이트\n",
    "    mean_train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "def test_step(model, loss_fn, mean_test_loss, test_accuracy, x, labels):\n",
    "    predictions = model(x)\n",
    "    loss_t = loss_fn(labels, predictions)   \n",
    "    \n",
    "    mean_test_loss(loss_t)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(f'dataset/{DATA_PATH}.npy', 'rb') as f:\n",
    "    x = np.load(f)\n",
    "    y = np.load(f)\n",
    "    \n",
    "# y = to_categorical(y, num_classes=len(CLASS_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 3s 37ms/step - loss: 1.4797 - accuracy: 0.4389 - val_loss: 2.0765 - val_accuracy: 0.2164\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 1.1562 - accuracy: 0.5636 - val_loss: 2.3648 - val_accuracy: 0.2164\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.9568 - accuracy: 0.6384 - val_loss: 2.1709 - val_accuracy: 0.1269\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.9069 - accuracy: 0.6534 - val_loss: 2.3635 - val_accuracy: 0.1343\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.7648 - accuracy: 0.7232 - val_loss: 1.9169 - val_accuracy: 0.2687\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6730 - accuracy: 0.7731 - val_loss: 2.3334 - val_accuracy: 0.2164\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 0.6200 - accuracy: 0.7631 - val_loss: 2.0016 - val_accuracy: 0.2388\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6308 - accuracy: 0.7980 - val_loss: 2.4058 - val_accuracy: 0.2388\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6453 - accuracy: 0.7556 - val_loss: 2.5172 - val_accuracy: 0.2612\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 10ms/step - loss: 0.6411 - accuracy: 0.7830 - val_loss: 2.2249 - val_accuracy: 0.3582\n"
     ]
    }
   ],
   "source": [
    "model = SERNET(len(CLASS_LABELS))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x, y, epochs=10, validation_split=0.25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth label operation\n",
    "def smooth_labels(labels, factor=0.1):\n",
    "    \"\"\"\n",
    "        smooth the labels\n",
    "        returned the smoothed labels\n",
    "    \"\"\"\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dataset_name, model_name, feature_name, fold, now_time):\n",
    "    save_path = os.path.join('Models', dataset_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    naming = f'{model_name}_{feature_name}_{fold}-fold_{now_time}'\n",
    "    \n",
    "    h5_path = f'{naming}.h5'\n",
    "    model.save_weights(os.path.join(save_path, h5_path))\n",
    "    \n",
    "    # json_path = f'{naming}.json'\n",
    "    # with open(os.path.join(save_path, json_path), \"w\") as json_file:\n",
    "    #     json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_DECAY_PARAMETERS = -0.15\n",
    "LEARNING_RATE_DECAY_STRATPOINT = 50\n",
    "LEARNING_RATE_DECAY_STEP = 20\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < LEARNING_RATE_DECAY_STRATPOINT:\n",
    "        return lr\n",
    "    else:\n",
    "        if epoch % LEARNING_RATE_DECAY_STEP == 0:\n",
    "            lr = lr * tf.math.exp(LEARNING_RATE_DECAY_PARAMETERS)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_list = []\n",
    "eva_matrix = []\n",
    "\n",
    "emotions_groundtruth_list = np.array([])\n",
    "predicted_emotions_list = np.array([])\n",
    "\n",
    "avg_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=k, shuffle=False, random_state=None)\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(x, y)), desc='Training {k}-Fold.....'):\n",
    "    now_time = datetime.now().strftime(\"%m-%d-%H%M%S\")\n",
    "    \n",
    "    x_train, y_train = x[train], y[train]\n",
    "    # y[train] = smooth_labels(y[train], 0.1)\n",
    "    \n",
    "    x_test, y_test = x[test], y[test]\n",
    "    \n",
    "    shape = x_train.shape[1:]\n",
    "    \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    \n",
    "    model = SERNET(len(CLASS_LABELS))\n",
    "    # model.build(input_shape=(None, shape[0], shape[1], shape[2]))\n",
    "    # print(model.summary())\n",
    "    \n",
    "    # metrics\n",
    "    mean_train_loss = Mean(name='train_loss')\n",
    "    train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    mean_test_loss = Mean(name='test_loss')\n",
    "    test_accuracy = SparseCategoricalAccuracy(name='test_accuracy')\n",
    "    \n",
    "    best_test_loss = 0x3f3f3f\n",
    "    best_test_acc = -1\n",
    "    \n",
    "    batch_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(2022).batch(BATCH)\n",
    "    batch_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH)\n",
    "    for epoch in tqdm(range(EPOCHS), desc=f'Fold-{i+1}'):\n",
    "        for features, labels in batch_train:\n",
    "            train_step(model, loss_fn, optimizer, mean_train_loss, train_accuracy, features, labels)\n",
    "        for features, labels in batch_test:\n",
    "            test_step(model, loss_fn, mean_test_loss, test_accuracy, features, labels)\n",
    "        \n",
    "        train_loss = mean_train_loss.result()\n",
    "        train_acc = train_accuracy.result()\n",
    "        test_loss = mean_test_loss.result()\n",
    "        test_acc = test_accuracy.result()\n",
    "        \n",
    "        cur_lr = K.eval(optimizer.lr)\n",
    "        print(f'{epoch+1}/{EPOCHS} lr={cur_lr:.5f} - loss:{train_loss:.3f}, acc:{train_acc:.3f}, test_loss:{test_loss:.3f}, test_acc:{test_acc:.3f}')\n",
    "        \n",
    "        set_lr = scheduler(epoch, K.eval(optimizer.lr))\n",
    "        K.set_value(optimizer.learning_rate, set_lr)\n",
    "        \n",
    "        if best_test_loss > test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_test_acc = test_acc\n",
    "            y_pred_best = model.predict(x[test])\n",
    "            \n",
    "            # model save\n",
    "            save_model(model, DATA_PATH, model_name, feature_name, i, now_time)\n",
    "            \n",
    "    print(f'[*] Done - loss:{best_test_loss:.3f}, acc:{best_test_acc:.3f}')\n",
    "    \n",
    "    avg_acc += best_test_acc\n",
    "    \n",
    "    conf_matrix = confusion_matrix(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1))\n",
    "    conf_matrix_list.append(conf_matrix)\n",
    "    \n",
    "    em = classification_report(np.argmax(y[test],axis=1),np.argmax(y_pred_best,axis=1), target_names=CLASS_LABELS,output_dict=True)\n",
    "    eva_matrix.append(em)\n",
    "    \n",
    "    emotions_groundtruth_list = np.append(emotions_groundtruth_list, np.argmax(y[test],axis=1))\n",
    "    predicted_emotions_list = np.append(predicted_emotions_list, np.argmax(y_pred_best,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Report = classification_report(emotions_groundtruth_list, predicted_emotions_list)\n",
    "\n",
    "os.makedirs(f'Results/{DATA_PATH}', exist_ok=True)\n",
    "report_path = f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold_nomalize.txt'\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "naming = f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold'\n",
    "naming = f'{naming}_{avg_acc/5:.3f}.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(naming)\n",
    "for i,item in enumerate(conf_matrix_list):\n",
    "    temp = {}\n",
    "    temp[\" \"] = CLASS_LABELS\n",
    "    j = 0\n",
    "    for j,l in enumerate(item):\n",
    "        temp[CLASS_LABELS[j]]=item[j]\n",
    "    data1 = pd.DataFrame(temp)\n",
    "    data1.to_excel(writer,sheet_name=str(i), encoding='utf8')\n",
    "    df = pd.DataFrame(eva_matrix[i]).transpose()\n",
    "    df.to_excel(writer,sheet_name=str(i)+\"_evaluate\", encoding='utf8')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('LIGHT-SERNET')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5069a7505627bbd03cc90b99af22c43f4aff4d8e01211d9e29e858623d083b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
