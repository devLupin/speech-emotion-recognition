{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from natsort import natsorted\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "data_root = 'data/EMO-DB_segmented'\n",
    "\n",
    "dict_emotions = {\n",
    "    'anger': 0,\n",
    "    'anxiety_fear': 1,\n",
    "    'boredom': 2,\n",
    "    'disgust': 3,\n",
    "    'happiness': 4,\n",
    "    'neutral': 5,\n",
    "    'sadness': 6\n",
    "}\n",
    "\n",
    "def prepare_EMODB():\n",
    "    dirs = os.listdir(data_root)\n",
    "    dirs = natsorted(dirs)\n",
    "    \n",
    "    paths, emotions, actors = [], [], []\n",
    "    \n",
    "    for d in dirs:\n",
    "        cur = os.path.join(data_root, d)\n",
    "        \n",
    "        cur_emotion = dict_emotions[d]\n",
    "        \n",
    "        cur_audios = os.listdir(cur)\n",
    "        cur_audios = natsorted(cur_audios)\n",
    "        for aud in cur_audios:\n",
    "            paths.append(os.path.join(cur, aud))\n",
    "            emotions.append(cur_emotion)\n",
    "            actors.append(aud[:2])\n",
    "    \n",
    "    return paths, emotions, actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, emotions, actors = prepare_EMODB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LENGTH = 1024\n",
    "FRAME_STEP = 256\n",
    "FFT_LENGTH=1024\n",
    "\n",
    "N_MFCC = 40\n",
    "\n",
    "NUM_SPECTROGRAM_BINS = 513\n",
    "NUM_MEL_BINS = 128\n",
    "LOWER_EDGE_HERTZ = 80.0\n",
    "UPPER_EDGE_HERTZ = 7600.0\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(NUM_MEL_BINS,\n",
    "\t\t\t\t\t\t\t\t\t                                NUM_SPECTROGRAM_BINS,\n",
    "\t\t\t\t\t\t\t\t\t                                SAMPLE_RATE,\n",
    "\t\t\t\t\t\t\t\t\t                                LOWER_EDGE_HERTZ,\n",
    "\t\t\t\t\t\t\t\t\t                                UPPER_EDGE_HERTZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio_binary):\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "def get_waveform(file_path):\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = []\n",
    "for p in paths:\n",
    "    waveforms.append(get_waveform(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535, 48000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveforms = np.array(waveforms)\n",
    "waveforms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=FRAME_LENGTH, frame_step=FRAME_STEP, fft_length=FFT_LENGTH)\n",
    "\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    return spectrogram\n",
    "\n",
    "def get_mel_spectrogram(spectrogram):\n",
    "    mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
    "\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "# def get_mfcc(waveform, clip_value=10):\n",
    "#     waveform = tf.cast(waveform, tf.float32)\n",
    "#     spectrogram = tf.raw_ops.AudioSpectrogram(input=waveform,\n",
    "#                                               window_size=FRAME_LENGTH,\n",
    "#                                               stride=FRAME_STEP,\n",
    "#                                               magnitude_squared=True,\n",
    "#                                              )\n",
    "    \n",
    "#     mfcc = tf.raw_ops.Mfcc(spectrogram=spectrogram,\n",
    "#                            sample_rate=SAMPLE_RATE,\n",
    "#                            upper_frequency_limit=UPPER_EDGE_HERTZ,\n",
    "#                            lower_frequency_limit=LOWER_EDGE_HERTZ,\n",
    "#                            filterbank_channel_count=NUM_MEL_BINS,\n",
    "#                            dct_coefficient_count=N_MFCC,\n",
    "#                           )\n",
    "#     return tf.clip_by_value(mfcc, -clip_value, clip_value)\n",
    "\n",
    "def get_mfcc(log_mel_spectrograms, clip_value=10):\n",
    "    mfcc = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :N_MFCC]\n",
    "\n",
    "    return tf.clip_by_value(mfcc, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features(audio, input_type=\"mfcc\", merge_tflite=False):\n",
    "    if input_type == \"spectrogram\":\n",
    "        spectrogram = get_spectrogram(audio)\n",
    "        return spectrogram\n",
    "    elif input_type == \"mel_spectrogram\":\n",
    "        spectrogram = get_spectrogram(audio)\n",
    "        mel_spectrogram = get_mel_spectrogram(spectrogram)\n",
    "        return mel_spectrogram\n",
    "    elif input_type == \"mfcc\":\n",
    "        if merge_tflite:\n",
    "            mfcc = get_mfcc(audio)[0]\n",
    "        else: \n",
    "            spectrogram = get_spectrogram(audio)\n",
    "            mel_spectrogram = get_mel_spectrogram(spectrogram)\n",
    "            mfcc = get_mfcc(mel_spectrogram)\n",
    "        return mfcc\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('input_type not valid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs = get_features(waveforms)\n",
    "len(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(features, emotions, actors):\n",
    "    data = []\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        data.append({\n",
    "            'feature': features[i],\n",
    "            'emotion': emotions[i],\n",
    "            'actor': actors[i]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_df(mfccs, emotions, actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test(fold, df, save_path=\"\"):\n",
    "    \"\"\"\n",
    "    Divide the data in train and test in a subject-wise 5-CV way. The division is generated before running the training\n",
    "    of each fold.\n",
    "    :param fold:[int] Fold to create the train and test sets [ranging from 0 - 4]\n",
    "    :param df:[DataFrame] Dataframe with the complete list of files generated\n",
    "    :param save_path:[str] Path to save the train.csv and test.csv per fold\n",
    "    \"\"\"\n",
    "    \n",
    "    actors_per_fold = {\n",
    "        0: ['03'],\n",
    "        1: ['08'],\n",
    "        2: ['09'],\n",
    "        3: ['10'],\n",
    "        4: ['11'],\n",
    "        5: ['12'],\n",
    "        6: ['13'],\n",
    "        7: ['14'],\n",
    "        8: ['15'],\n",
    "        9: ['16']\n",
    "    }\n",
    "\n",
    "    test_df = df.loc[df['actor'].isin(actors_per_fold[fold])]\n",
    "    train_df = df.loc[~df['actor'].isin(actors_per_fold[fold])]\n",
    "\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    X_train = np.array([data for data in train_df['feature']])\n",
    "    y_train = np.array([data for data in train_df['emotion']])\n",
    "    X_test = np.array([data for data in test_df['feature']])\n",
    "    y_test = np.array([data for data in test_df['emotion']])\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_test = np.expand_dims(X_test, 1)\n",
    "    \n",
    "    with open(save_path+'.npy', 'wb') as f:\n",
    "        np.save(f, X_train)\n",
    "        np.save(f, y_train)\n",
    "        np.save(f, X_test)\n",
    "        np.save(f, y_test)\n",
    "        \n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(486, 1, 184, 40) (486,) (49, 1, 184, 40) (49,)\n",
      "(477, 1, 184, 40) (477,) (58, 1, 184, 40) (58,)\n",
      "(492, 1, 184, 40) (492,) (43, 1, 184, 40) (43,)\n",
      "(497, 1, 184, 40) (497,) (38, 1, 184, 40) (38,)\n",
      "(480, 1, 184, 40) (480,) (55, 1, 184, 40) (55,)\n",
      "(500, 1, 184, 40) (500,) (35, 1, 184, 40) (35,)\n",
      "(474, 1, 184, 40) (474,) (61, 1, 184, 40) (61,)\n",
      "(466, 1, 184, 40) (466,) (69, 1, 184, 40) (69,)\n",
      "(479, 1, 184, 40) (479,) (56, 1, 184, 40) (56,)\n",
      "(464, 1, 184, 40) (464,) (71, 1, 184, 40) (71,)\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "    save_root = 'LIGHT-SERNET dataset'\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "    \n",
    "    save_path = os.path.join(save_root, \"fold\"+str(fold))\n",
    "    \n",
    "    generate_train_test(fold, df, save_path)\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('LIGHT-SERNET')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5069a7505627bbd03cc90b99af22c43f4aff4d8e01211d9e29e858623d083b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
