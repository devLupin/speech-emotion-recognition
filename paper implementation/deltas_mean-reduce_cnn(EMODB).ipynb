{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Lambda, Conv2D, Dropout,Dense,Activation,Input,GlobalAveragePooling1D, Concatenate, GlobalAveragePooling2D, LayerNormalization\n",
    "from tensorflow.keras.layers import Reshape,Flatten,BatchNormalization,MaxPooling1D,AveragePooling2D,Reshape,Attention, ReLU, Activation, SpatialDropout2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from Config import Config\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Mean, CategoricalAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'TIMNET-dataset'\n",
    "DATA_PATH = 'EMODB_deltas-mean'\n",
    "CLASS_LABELS = Config.EMODB_LABELS\n",
    "k = 10\n",
    "\n",
    "model_name = 'reduce-cnn_deltas-mean'\n",
    "feature_name = 'mfcc'\n",
    "\n",
    "learning_rate=0.001\n",
    "beta_1=0.975\n",
    "beta_2=0.932\n",
    "epsilon=1e-8\n",
    "\n",
    "EPOCHS = 300\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCapssquash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "        The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "        :param vectors: some vectors to be squashed, N-dim tensor\n",
    "        :param axis: the axis to squash\n",
    "        :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCap(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, dim_capsule=64, n_channels=3, kernel_size=3, strides=1, padding='valid'):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.conv = Sequential([\n",
    "        Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding, name='primarycap_conv2d'),\n",
    "        Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape'),\n",
    "        Lambda(function=PrimaryCapssquash, name='primarycap_squash')\n",
    "    ])\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.conv(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DigitCapssquash(Value, axis = -1):\n",
    "    \"\"\"\n",
    "        Squash activation in PrimaryCaps\n",
    "        :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    Square_Vector = K.sum(K.square(Value), axis, keepdims=True)\n",
    "    Proportion = Square_Vector / (1 + Square_Vector) / K.sqrt(Square_Vector + K.epsilon())\n",
    "    Output = Proportion * Value\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"\n",
    "        softmax in Dynamic Routings\n",
    "    \"\"\" \n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex/K.sum(ex, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capsule(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        self.activation = DigitCapssquash\n",
    "        \n",
    "    def get_config(self):\n",
    "       config = {\"num_capsule\":self.num_capsule,\n",
    "                 \"dim_capsule\":self.dim_capsule,\n",
    "                 \"routings\":self.routings,\n",
    "                 \"share_weights\":self.share_weights,\n",
    "                 \"activation\":self.activation\n",
    "                }\n",
    "       base_config = super(Capsule, self).get_config()\n",
    "       return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        #input_dim_capsule = 8\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,#input_dim_capsule = 16\n",
    "                                            self.num_capsule * self.dim_capsule), #16*32\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "    \n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:,:,:,0])\n",
    "\n",
    "        for i in range(self.routings): #Routings\n",
    "            c = softmax(b, 1)\n",
    "\n",
    "            o = tf.einsum('bin,binj->bij', c, u_hat_vecs)\n",
    "            if K.backend() == 'theano':\n",
    "                o = K.sum(o, axis=1)\n",
    "            if i < self.routings - 1:\n",
    "                o = K.l2_normalize(o, -1)\n",
    "                b = tf.einsum('bij,binj->bin', o, u_hat_vecs)\n",
    "                if K.backend() == 'theano':\n",
    "                    b = K.sum(b, axis=1)\n",
    "\n",
    "        return self.activation(o)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTL_MTNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.conv1 = Sequential([\n",
    "      Conv2D(filters=32, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.2),\n",
    "      \n",
    "      Conv2D(filters=48, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.2),\n",
    "      \n",
    "      Conv2D(filters=64, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.5)\n",
    "    ])\n",
    "    \n",
    "    self.conv2 = Sequential([\n",
    "      Conv2D(filters=32, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.2),\n",
    "      \n",
    "      Conv2D(filters=48, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.2),\n",
    "      \n",
    "      Conv2D(filters=64, kernel_size=3),\n",
    "      BatchNormalization(axis=1),\n",
    "      Activation('elu'),\n",
    "      AveragePooling2D(),\n",
    "      SpatialDropout2D(0.5)\n",
    "    ])\n",
    "    \n",
    "    self.primarycap = PrimaryCap()\n",
    "    \n",
    "    self.attention = Attention(use_scale=True)\n",
    "    self.LN = LayerNormalization()\n",
    "    \n",
    "    self.lamb = Lambda(lambda x: tf.multiply(x[0], x[1]))\n",
    "    \n",
    "    self.capsule = Capsule(6,64,3,True)\n",
    "    self.gap = GlobalAveragePooling1D()\n",
    "    self.dropout = Dropout(0.2)\n",
    "    self.classifier = Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    \n",
    "  def call(self, inputs):\n",
    "    forward = inputs\n",
    "    backward =tf.reverse(inputs, axis=[2])\n",
    "    \n",
    "    x1 = self.conv1(forward)\n",
    "    x2 = self.conv2(backward)\n",
    "    \n",
    "    cap = x1 + x2\n",
    "    \n",
    "    primarycaps = self.primarycap(cap)\n",
    "    \n",
    "    cap = primarycaps\n",
    "    \n",
    "    sa = self.attention([primarycaps, primarycaps, primarycaps])\n",
    "    sa = self.LN(sa)\n",
    "    \n",
    "    cap = self.lamb([cap, sa])\n",
    "    \n",
    "    capsule = self.capsule(cap)\n",
    "    gap = self.gap(capsule)\n",
    "    drop = self.dropout(gap)\n",
    "    \n",
    "    output_softmax = self.classifier(drop)\n",
    "    return output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ctl_mt_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 22, 3, 64)         43240     \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 22, 3, 64)         43240     \n",
      "                                                                 \n",
      " primary_cap (PrimaryCap)    multiple                  110784    \n",
      "                                                                 \n",
      " attention (Attention)       multiple                  1         \n",
      "                                                                 \n",
      " layer_normalization (LayerN  multiple                 128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lambda (Lambda)             multiple                  0         \n",
      "                                                                 \n",
      " capsule (Capsule)           multiple                  24576     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  multiple                 0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,424\n",
      "Trainable params: 221,088\n",
      "Non-trainable params: 1,336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CTL_MTNet(len(CLASS_LABELS))\n",
    "model.build(input_shape=(None, 196, 39, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Margin Loss\n",
    "        :param y_true: [None, n_classes]\n",
    "        :param y_pred: [None, num_capsule]\n",
    "        :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, x, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 미분 계산\n",
    "        predictions = model(x, training=True)\n",
    "        loss1 = margin_loss(labels, predictions)\n",
    "        loss2 = MeanSquaredError()(labels, predictions)\n",
    "        \n",
    "        loss = loss1*1. + loss2*0.392\n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))     # 신경망 파라미터 업데이트\n",
    "    \n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    acc.update_state(labels, predictions)\n",
    "    accuracy = acc.result().numpy()\n",
    "    \n",
    "    return loss, accuracy*100\n",
    "\n",
    "def test_step(model, x, labels):\n",
    "    predictions = model(x)\n",
    "    loss1 = margin_loss(labels, predictions)\n",
    "    loss2 = MeanSquaredError()(labels, predictions)\n",
    "    loss = loss1*1. + loss2*0.392\n",
    "    \n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    acc.update_state(labels, predictions)\n",
    "    accuracy = acc.result().numpy()\n",
    "    \n",
    "    return loss, accuracy*100, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "with open(f'{DATA_ROOT}/{DATA_PATH}.npy', 'rb') as f:\n",
    "    x = np.load(f)\n",
    "    y = np.load(f)\n",
    "\n",
    "y = to_categorical(y,num_classes=len(CLASS_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_DECAY_PARAMETERS = -0.15\n",
    "LEARNING_RATE_DECAY_STRATPOINT = 50\n",
    "LEARNING_RATE_DECAY_STEP = 20\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < LEARNING_RATE_DECAY_STRATPOINT:\n",
    "        return lr\n",
    "    else:\n",
    "        if epoch % LEARNING_RATE_DECAY_STEP == 0:\n",
    "            lr = lr * tf.math.exp(LEARNING_RATE_DECAY_PARAMETERS)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth label operation\n",
    "def smooth_labels(labels, factor=0.1):\n",
    "    \"\"\"\n",
    "        smooth the labels\n",
    "        returned the smoothed labels\n",
    "    \"\"\"\n",
    "    labels *= (1 - factor)\n",
    "    labels += (factor / labels.shape[1])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discord_notice import start, end\n",
    "# start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821dd9a3165d4d8b8a1bfcb074f02fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130ce958b48a4b128261792c534b1641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300 lr=0.00100 - loss:0.606, acc:26.562, val_loss:0.588, val_acc:37.642\n",
      "Best loss:4144959.000, Best accuracy:-1.000, Best F1-score:-1.000\n",
      "2/300 lr=0.00100 - loss:0.579, acc:35.547, val_loss:0.561, val_acc:43.040\n",
      "Best loss:0.588, Best accuracy:37.642, Best F1-score:0.248\n",
      "3/300 lr=0.00100 - loss:0.562, acc:33.594, val_loss:0.544, val_acc:39.915\n",
      "Best loss:0.561, Best accuracy:43.040, Best F1-score:0.344\n",
      "4/300 lr=0.00100 - loss:0.532, acc:43.555, val_loss:0.523, val_acc:43.750\n",
      "Best loss:0.561, Best accuracy:43.040, Best F1-score:0.344\n",
      "5/300 lr=0.00100 - loss:0.526, acc:44.922, val_loss:0.512, val_acc:43.040\n",
      "Best loss:0.523, Best accuracy:43.750, Best F1-score:0.335\n",
      "6/300 lr=0.00100 - loss:0.509, acc:48.047, val_loss:0.496, val_acc:44.602\n",
      "Best loss:0.523, Best accuracy:43.750, Best F1-score:0.335\n",
      "7/300 lr=0.00100 - loss:0.496, acc:46.484, val_loss:0.492, val_acc:44.602\n",
      "Best loss:0.496, Best accuracy:44.602, Best F1-score:0.314\n",
      "8/300 lr=0.00100 - loss:0.485, acc:48.828, val_loss:0.469, val_acc:43.040\n",
      "Best loss:0.496, Best accuracy:44.602, Best F1-score:0.314\n",
      "9/300 lr=0.00100 - loss:0.492, acc:43.945, val_loss:0.466, val_acc:49.290\n",
      "Best loss:0.496, Best accuracy:44.602, Best F1-score:0.314\n",
      "10/300 lr=0.00100 - loss:0.471, acc:52.539, val_loss:0.447, val_acc:55.398\n",
      "Best loss:0.466, Best accuracy:49.290, Best F1-score:0.416\n",
      "11/300 lr=0.00100 - loss:0.488, acc:45.312, val_loss:0.449, val_acc:47.017\n",
      "Best loss:0.447, Best accuracy:55.398, Best F1-score:0.480\n",
      "12/300 lr=0.00100 - loss:0.474, acc:50.391, val_loss:0.433, val_acc:49.290\n",
      "Best loss:0.447, Best accuracy:55.398, Best F1-score:0.480\n",
      "13/300 lr=0.00100 - loss:0.444, acc:56.250, val_loss:0.427, val_acc:51.562\n",
      "Best loss:0.447, Best accuracy:55.398, Best F1-score:0.480\n",
      "14/300 lr=0.00100 - loss:0.446, acc:50.000, val_loss:0.416, val_acc:51.562\n",
      "Best loss:0.447, Best accuracy:55.398, Best F1-score:0.480\n",
      "15/300 lr=0.00100 - loss:0.452, acc:48.633, val_loss:0.414, val_acc:61.506\n",
      "Best loss:0.447, Best accuracy:55.398, Best F1-score:0.480\n",
      "16/300 lr=0.00100 - loss:0.418, acc:59.766, val_loss:0.400, val_acc:57.670\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "17/300 lr=0.00100 - loss:0.436, acc:51.953, val_loss:0.396, val_acc:54.688\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "18/300 lr=0.00100 - loss:0.433, acc:51.172, val_loss:0.394, val_acc:59.233\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "19/300 lr=0.00100 - loss:0.409, acc:57.812, val_loss:0.404, val_acc:60.795\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "20/300 lr=0.00100 - loss:0.413, acc:55.469, val_loss:0.370, val_acc:56.250\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "21/300 lr=0.00100 - loss:0.411, acc:57.227, val_loss:0.401, val_acc:59.233\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "22/300 lr=0.00100 - loss:0.397, acc:61.719, val_loss:0.367, val_acc:56.960\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "23/300 lr=0.00100 - loss:0.393, acc:59.375, val_loss:0.397, val_acc:58.523\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "24/300 lr=0.00100 - loss:0.430, acc:54.883, val_loss:0.346, val_acc:58.523\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "25/300 lr=0.00100 - loss:0.403, acc:56.445, val_loss:0.386, val_acc:59.943\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "26/300 lr=0.00100 - loss:0.379, acc:63.086, val_loss:0.346, val_acc:67.756\n",
      "Best loss:0.414, Best accuracy:61.506, Best F1-score:0.586\n",
      "27/300 lr=0.00100 - loss:0.372, acc:65.820, val_loss:0.369, val_acc:65.483\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "28/300 lr=0.00100 - loss:0.412, acc:58.984, val_loss:0.350, val_acc:66.193\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "29/300 lr=0.00100 - loss:0.391, acc:61.328, val_loss:0.347, val_acc:63.210\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "30/300 lr=0.00100 - loss:0.347, acc:67.773, val_loss:0.331, val_acc:62.358\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "31/300 lr=0.00100 - loss:0.354, acc:68.164, val_loss:0.335, val_acc:65.483\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "32/300 lr=0.00100 - loss:0.362, acc:62.695, val_loss:0.319, val_acc:63.210\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "33/300 lr=0.00100 - loss:0.349, acc:69.336, val_loss:0.345, val_acc:59.375\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "34/300 lr=0.00100 - loss:0.345, acc:70.312, val_loss:0.312, val_acc:68.608\n",
      "Best loss:0.346, Best accuracy:67.756, Best F1-score:0.637\n",
      "35/300 lr=0.00100 - loss:0.326, acc:70.117, val_loss:0.326, val_acc:66.193\n",
      "Best loss:0.312, Best accuracy:68.608, Best F1-score:0.649\n",
      "36/300 lr=0.00100 - loss:0.352, acc:68.164, val_loss:0.302, val_acc:64.773\n",
      "Best loss:0.312, Best accuracy:68.608, Best F1-score:0.649\n",
      "37/300 lr=0.00100 - loss:0.340, acc:70.117, val_loss:0.294, val_acc:66.903\n",
      "Best loss:0.312, Best accuracy:68.608, Best F1-score:0.649\n",
      "38/300 lr=0.00100 - loss:0.327, acc:71.094, val_loss:0.300, val_acc:67.045\n",
      "Best loss:0.312, Best accuracy:68.608, Best F1-score:0.649\n",
      "39/300 lr=0.00100 - loss:0.341, acc:65.234, val_loss:0.291, val_acc:70.881\n",
      "Best loss:0.312, Best accuracy:68.608, Best F1-score:0.649\n",
      "40/300 lr=0.00100 - loss:0.347, acc:64.648, val_loss:0.308, val_acc:68.608\n",
      "Best loss:0.291, Best accuracy:70.881, Best F1-score:0.652\n",
      "41/300 lr=0.00100 - loss:0.352, acc:65.234, val_loss:0.290, val_acc:59.233\n",
      "Best loss:0.291, Best accuracy:70.881, Best F1-score:0.652\n",
      "42/300 lr=0.00100 - loss:0.334, acc:65.234, val_loss:0.277, val_acc:71.591\n",
      "Best loss:0.291, Best accuracy:70.881, Best F1-score:0.652\n",
      "43/300 lr=0.00100 - loss:0.355, acc:66.797, val_loss:0.263, val_acc:76.989\n",
      "Best loss:0.277, Best accuracy:71.591, Best F1-score:0.691\n",
      "44/300 lr=0.00100 - loss:0.312, acc:74.023, val_loss:0.245, val_acc:77.699\n",
      "Best loss:0.263, Best accuracy:76.989, Best F1-score:0.736\n",
      "45/300 lr=0.00100 - loss:0.332, acc:64.258, val_loss:0.285, val_acc:70.028\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "46/300 lr=0.00100 - loss:0.309, acc:67.578, val_loss:0.260, val_acc:73.722\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "47/300 lr=0.00100 - loss:0.303, acc:72.070, val_loss:0.279, val_acc:70.739\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "48/300 lr=0.00100 - loss:0.322, acc:68.945, val_loss:0.252, val_acc:76.989\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "49/300 lr=0.00100 - loss:0.316, acc:67.383, val_loss:0.249, val_acc:71.449\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "50/300 lr=0.00100 - loss:0.291, acc:75.586, val_loss:0.230, val_acc:76.847\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "51/300 lr=0.00100 - loss:0.323, acc:70.117, val_loss:0.212, val_acc:77.699\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "52/300 lr=0.00100 - loss:0.298, acc:73.242, val_loss:0.224, val_acc:79.261\n",
      "Best loss:0.245, Best accuracy:77.699, Best F1-score:0.739\n",
      "53/300 lr=0.00100 - loss:0.291, acc:74.805, val_loss:0.231, val_acc:76.136\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "54/300 lr=0.00100 - loss:0.302, acc:74.023, val_loss:0.245, val_acc:73.153\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "55/300 lr=0.00100 - loss:0.297, acc:71.680, val_loss:0.252, val_acc:70.028\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "56/300 lr=0.00100 - loss:0.280, acc:74.023, val_loss:0.237, val_acc:73.011\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "57/300 lr=0.00100 - loss:0.274, acc:76.758, val_loss:0.222, val_acc:77.699\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "58/300 lr=0.00100 - loss:0.288, acc:71.875, val_loss:0.211, val_acc:78.409\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "59/300 lr=0.00100 - loss:0.276, acc:77.344, val_loss:0.206, val_acc:79.972\n",
      "Best loss:0.224, Best accuracy:79.261, Best F1-score:0.780\n",
      "60/300 lr=0.00100 - loss:0.269, acc:77.539, val_loss:0.219, val_acc:78.409\n",
      "Best loss:0.206, Best accuracy:79.972, Best F1-score:0.793\n",
      "61/300 lr=0.00100 - loss:0.265, acc:77.930, val_loss:0.244, val_acc:73.864\n",
      "Best loss:0.206, Best accuracy:79.972, Best F1-score:0.793\n",
      "62/300 lr=0.00086 - loss:0.267, acc:76.172, val_loss:0.220, val_acc:83.097\n",
      "Best loss:0.206, Best accuracy:79.972, Best F1-score:0.793\n",
      "63/300 lr=0.00086 - loss:0.280, acc:77.734, val_loss:0.217, val_acc:83.097\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "64/300 lr=0.00086 - loss:0.253, acc:79.297, val_loss:0.246, val_acc:73.011\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "65/300 lr=0.00086 - loss:0.256, acc:79.688, val_loss:0.231, val_acc:74.574\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "66/300 lr=0.00086 - loss:0.250, acc:80.469, val_loss:0.239, val_acc:76.136\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "67/300 lr=0.00086 - loss:0.279, acc:74.609, val_loss:0.216, val_acc:77.699\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "68/300 lr=0.00086 - loss:0.260, acc:77.930, val_loss:0.206, val_acc:79.972\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "69/300 lr=0.00086 - loss:0.255, acc:79.297, val_loss:0.263, val_acc:67.614\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "70/300 lr=0.00086 - loss:0.244, acc:78.320, val_loss:0.206, val_acc:78.409\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "71/300 lr=0.00086 - loss:0.250, acc:78.516, val_loss:0.192, val_acc:79.972\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "72/300 lr=0.00086 - loss:0.242, acc:82.031, val_loss:0.243, val_acc:72.301\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "73/300 lr=0.00086 - loss:0.259, acc:75.195, val_loss:0.191, val_acc:79.972\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "74/300 lr=0.00086 - loss:0.240, acc:81.641, val_loss:0.208, val_acc:75.994\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "75/300 lr=0.00086 - loss:0.247, acc:81.055, val_loss:0.199, val_acc:79.261\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "76/300 lr=0.00086 - loss:0.216, acc:84.180, val_loss:0.224, val_acc:70.028\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "77/300 lr=0.00086 - loss:0.232, acc:83.789, val_loss:0.189, val_acc:79.972\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "78/300 lr=0.00086 - loss:0.228, acc:83.398, val_loss:0.182, val_acc:83.807\n",
      "Best loss:0.220, Best accuracy:83.097, Best F1-score:0.814\n",
      "79/300 lr=0.00086 - loss:0.221, acc:82.617, val_loss:0.200, val_acc:78.409\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "80/300 lr=0.00086 - loss:0.223, acc:82.812, val_loss:0.203, val_acc:78.409\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "81/300 lr=0.00086 - loss:0.222, acc:83.203, val_loss:0.212, val_acc:76.989\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "82/300 lr=0.00074 - loss:0.220, acc:82.031, val_loss:0.195, val_acc:79.972\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "83/300 lr=0.00074 - loss:0.233, acc:76.758, val_loss:0.208, val_acc:76.847\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "84/300 lr=0.00074 - loss:0.219, acc:84.570, val_loss:0.184, val_acc:82.244\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "85/300 lr=0.00074 - loss:0.213, acc:83.398, val_loss:0.186, val_acc:76.136\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "86/300 lr=0.00074 - loss:0.213, acc:84.570, val_loss:0.222, val_acc:73.722\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "87/300 lr=0.00074 - loss:0.208, acc:85.156, val_loss:0.193, val_acc:78.409\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "88/300 lr=0.00074 - loss:0.203, acc:86.328, val_loss:0.229, val_acc:76.136\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "89/300 lr=0.00074 - loss:0.218, acc:85.547, val_loss:0.212, val_acc:73.864\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "90/300 lr=0.00074 - loss:0.204, acc:85.352, val_loss:0.190, val_acc:76.136\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "91/300 lr=0.00074 - loss:0.199, acc:86.133, val_loss:0.184, val_acc:79.119\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "92/300 lr=0.00074 - loss:0.199, acc:86.328, val_loss:0.171, val_acc:79.972\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "93/300 lr=0.00074 - loss:0.193, acc:87.500, val_loss:0.163, val_acc:79.119\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "94/300 lr=0.00074 - loss:0.198, acc:86.719, val_loss:0.155, val_acc:85.227\n",
      "Best loss:0.182, Best accuracy:83.807, Best F1-score:0.834\n",
      "95/300 lr=0.00074 - loss:0.195, acc:86.523, val_loss:0.179, val_acc:79.830\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "96/300 lr=0.00074 - loss:0.229, acc:80.469, val_loss:0.177, val_acc:80.682\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "97/300 lr=0.00074 - loss:0.197, acc:86.523, val_loss:0.202, val_acc:76.847\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "98/300 lr=0.00074 - loss:0.179, acc:89.648, val_loss:0.189, val_acc:80.682\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "99/300 lr=0.00074 - loss:0.212, acc:79.688, val_loss:0.183, val_acc:79.972\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "100/300 lr=0.00074 - loss:0.209, acc:84.766, val_loss:0.196, val_acc:76.847\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "101/300 lr=0.00074 - loss:0.185, acc:87.305, val_loss:0.172, val_acc:83.807\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "102/300 lr=0.00064 - loss:0.251, acc:80.859, val_loss:0.193, val_acc:78.409\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "103/300 lr=0.00064 - loss:0.186, acc:87.695, val_loss:0.216, val_acc:76.136\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "104/300 lr=0.00064 - loss:0.187, acc:88.477, val_loss:0.186, val_acc:78.409\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "105/300 lr=0.00064 - loss:0.208, acc:82.812, val_loss:0.203, val_acc:74.574\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "106/300 lr=0.00064 - loss:0.177, acc:89.453, val_loss:0.211, val_acc:69.176\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "107/300 lr=0.00064 - loss:0.197, acc:82.227, val_loss:0.194, val_acc:75.284\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "108/300 lr=0.00064 - loss:0.194, acc:87.109, val_loss:0.195, val_acc:73.011\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "109/300 lr=0.00064 - loss:0.222, acc:80.469, val_loss:0.218, val_acc:74.574\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "110/300 lr=0.00064 - loss:0.197, acc:84.375, val_loss:0.211, val_acc:73.011\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "111/300 lr=0.00064 - loss:0.191, acc:86.914, val_loss:0.189, val_acc:80.682\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "112/300 lr=0.00064 - loss:0.186, acc:88.281, val_loss:0.203, val_acc:75.994\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "113/300 lr=0.00064 - loss:0.177, acc:89.453, val_loss:0.194, val_acc:75.284\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "114/300 lr=0.00064 - loss:0.187, acc:88.086, val_loss:0.176, val_acc:78.409\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "115/300 lr=0.00064 - loss:0.180, acc:90.234, val_loss:0.202, val_acc:75.284\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "116/300 lr=0.00064 - loss:0.193, acc:83.594, val_loss:0.189, val_acc:73.011\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "117/300 lr=0.00064 - loss:0.199, acc:88.086, val_loss:0.203, val_acc:73.011\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "118/300 lr=0.00064 - loss:0.179, acc:89.062, val_loss:0.182, val_acc:80.682\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "119/300 lr=0.00064 - loss:0.183, acc:88.477, val_loss:0.181, val_acc:79.119\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "120/300 lr=0.00064 - loss:0.167, acc:90.430, val_loss:0.174, val_acc:79.972\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "121/300 lr=0.00064 - loss:0.228, acc:83.594, val_loss:0.165, val_acc:82.955\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n",
      "122/300 lr=0.00055 - loss:0.175, acc:90.430, val_loss:0.154, val_acc:82.955\n",
      "Best loss:0.155, Best accuracy:85.227, Best F1-score:0.842\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from keras.models import load_model\n",
    "\n",
    "emotions_groundtruth_list = np.array([])\n",
    "predicted_emotions_list = np.array([])\n",
    "\n",
    "fold_acc = []\n",
    "\n",
    "kfold = KFold(n_splits=k, shuffle=True, random_state=98)\n",
    "\n",
    "for i, (train, test) in tqdm(enumerate(kfold.split(x, y)), desc=f'Training {k}-Fold.....'):\n",
    "    now_time = datetime.now().strftime(\"%m-%d-%H%M%S\")\n",
    "    \n",
    "    save_path = f'Models/{DATA_PATH}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    h5_path = f'{save_path}/{model_name}_{i}-fold_.h5'\n",
    "    \n",
    "    x_train, y_train = x[train], y[train]\n",
    "    y_train = smooth_labels(y[train], 0.1)\n",
    "    \n",
    "    x_test, y_test = x[test], y[test]\n",
    "    \n",
    "    shape = x_train.shape[1:]\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "    \n",
    "    model = CTL_MTNet(len(CLASS_LABELS))\n",
    "    \n",
    "    best_test_loss = 0x3f3f3f\n",
    "    best_test_acc = -1\n",
    "    best_test_f1 = -1\n",
    "    \n",
    "    epoch_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    batch_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(2022).batch(BATCH)\n",
    "    batch_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH)\n",
    "    for epoch in tqdm(range(EPOCHS), desc=f'Fold-{i+1}'):\n",
    "\n",
    "        train_loss, train_acc = [], []\n",
    "        for features, labels in batch_train:\n",
    "            loss, acc = train_step(model, optimizer, features, labels)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        test_loss, test_acc, f1s = [], [], []\n",
    "        for features, labels in batch_test:\n",
    "            loss, acc, pred = test_step(model, features, labels)\n",
    "            test_loss.append(loss)\n",
    "            test_acc.append(acc)\n",
    "            \n",
    "            f1_metric = tfa.metrics.F1Score(num_classes=len(CLASS_LABELS), average='weighted')\n",
    "            f1_metric.update_state(labels, pred)\n",
    "            f1 = f1_metric.result().numpy()\n",
    "            f1s.append(f1)\n",
    "            \n",
    "        \n",
    "        epoch_loss = sum(train_loss)/len(train_loss)\n",
    "        epoch_acc = sum(train_acc)/len(train_acc)\n",
    "        val_loss = sum(test_loss)/len(test_loss)\n",
    "        val_acc = sum(test_acc)/len(test_acc)\n",
    "        f1_score = sum(f1s)/len(f1s)\n",
    "        \n",
    "        epoch_losses.append(epoch_loss)\n",
    "        valid_losses.append(val_loss)\n",
    "        \n",
    "        cur_lr = K.eval(optimizer.lr)\n",
    "        print(f'{epoch+1}/{EPOCHS} lr={cur_lr:.5f} - loss:{epoch_loss:.3f}, acc:{epoch_acc:.3f}, val_loss:{val_loss:.3f}, val_acc:{val_acc:.3f}')\n",
    "        print(f'Best loss:{best_test_loss:.3f}, Best accuracy:{best_test_acc:.3f}, Best F1-score:{best_test_f1:.3f}')\n",
    "        \n",
    "        set_lr = scheduler(epoch, K.eval(optimizer.lr))\n",
    "        K.set_value(optimizer.learning_rate, set_lr)\n",
    "        \n",
    "        if best_test_acc < val_acc:\n",
    "            best_test_acc = val_acc\n",
    "            best_test_loss = val_loss\n",
    "            best_test_f1 = f1_score\n",
    "            model.save_weights(h5_path)\n",
    "            \n",
    "            \n",
    "    model = CTL_MTNet(len(CLASS_LABELS))\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    model.load_weights(h5_path)\n",
    "    best_pred = model(x_test, training=False)\n",
    "    emotions_groundtruth_list = np.append(emotions_groundtruth_list, np.argmax(y_test, axis=1))\n",
    "    predicted_emotions_list = np.append(predicted_emotions_list, np.argmax(best_pred, axis=1))\n",
    "    \n",
    "    \n",
    "    print(f'[*] Done - acc:{best_test_acc:.3f}')\n",
    "    \n",
    "    plt.title('Loss Curve')\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.plot(epoch_losses[:],'b')\n",
    "    plt.plot(valid_losses[:],'r')\n",
    "    plt.legend(['Training loss','Validation loss'])\n",
    "    plt.show()\n",
    "    \n",
    "    fold_acc.append(best_test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'max:{max(fold_acc)}, min:{min(fold_acc)}, average:{sum(fold_acc)/len(fold_acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "Report = classification_report(emotions_groundtruth_list, predicted_emotions_list)\n",
    "\n",
    "os.makedirs(f'Results/{DATA_PATH}', exist_ok=True)\n",
    "report_path = f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold_nomalize.txt'\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "emotion_names = CLASS_LABELS\n",
    "\n",
    "\n",
    "# build confusion matrix and normalized confusion matrix\n",
    "conf_matrix = confusion_matrix(emotions_groundtruth_list, predicted_emotions_list)\n",
    "conf_matrix_norm = confusion_matrix(emotions_groundtruth_list, predicted_emotions_list,normalize='true')\n",
    "\n",
    "# make a confusion matrix with labels using a DataFrame\n",
    "confmatrix_df = pd.DataFrame(conf_matrix, index=emotion_names, columns=emotion_names)\n",
    "confmatrix_df_norm = pd.DataFrame(conf_matrix_norm, index=emotion_names, columns=emotion_names)\n",
    "\n",
    "# plot confusion matrices\n",
    "plt.figure(figsize=(16,6))\n",
    "sn.set(font_scale=1.8) # emotion label and title size\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df, annot=True, annot_kws={\"size\": 13}, fmt='g') #annot_kws is value font\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "sn.heatmap(confmatrix_df_norm, annot=True, annot_kws={\"size\": 13}) #annot_kws is value font\n",
    "plt.savefig(f'Results/{DATA_PATH}/{model_name}_{feature_name}_{k}-fold_confmatrix.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metric_calc(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:42:03) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cc79f3bff38b9826e331232bfe618f732509f3c8555218b497f1264fcaa8b42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
