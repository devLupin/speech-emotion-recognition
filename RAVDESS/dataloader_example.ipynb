{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import warnings; warnings.filterwarnings('ignore') #matplot lib complains about librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'features+labels.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/devlupin/ser/dataLoader_version.ipynb 셀 2\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfeatures+labels.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# open file in read mode and read data \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     X_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     X_valid \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features+labels.npy'"
     ]
    }
   ],
   "source": [
    "##### LOAD #########\n",
    "# choose load file name \n",
    "filename = 'features+labels.npy'\n",
    "\n",
    "# open file in read mode and read data \n",
    "with open(filename, 'rb') as f:\n",
    "    X_train = np.load(f)\n",
    "    X_valid = np.load(f)\n",
    "    X_test = np.load(f)\n",
    "    y_train = np.load(f)\n",
    "    y_valid = np.load(f)\n",
    "    y_test = np.load(f)\n",
    "\n",
    "# Check that we've recovered the right data\n",
    "print(f'X_train:{X_train.shape}, y_train:{y_train.shape}')\n",
    "print(f'X_valid:{X_valid.shape}, y_valid:{y_valid.shape}')\n",
    "print(f'X_test:{X_test.shape}, y_test:{y_test.shape}') \n",
    "\n",
    "\"\"\"\n",
    "(4320, 1, 40, 282)\n",
    "- 4320 : num. samples\n",
    "- 1 : channel(흑백)\n",
    "- 40 : mel pitch ranges, MFC 계수\n",
    "- 282 : time stamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change nn.sequential to take dict to make more readable \n",
    "\n",
    "class parallel_all_you_want(nn.Module):\n",
    "    # Define all layers present in the network\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__() \n",
    "        \n",
    "        ################ TRANSFORMER BLOCK #############################\n",
    "        # maxpool the input feature map/tensor to the transformer \n",
    "        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor\n",
    "        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])\n",
    "        \n",
    "        # define single transformer encoder layer\n",
    "        # self-attention + feedforward network from \"Attention is All You Need\" paper\n",
    "        # 4 multi-head self-attention layers each with 40-->512--->40 feedforward network\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=40, # input feature (frequency) dim after maxpooling 40*282 -> 40*70 (MFC*time)\n",
    "            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block\n",
    "            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40-->512--->40\n",
    "            dropout=0.4, \n",
    "            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time\n",
    "        )\n",
    "        \n",
    "        # I'm using 4 instead of the 6 identical stacked encoder layrs used in Attention is All You Need paper\n",
    "        # Complete transformer block contains 4 full transformer encoder layers (each w/ multihead self-attention+feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)\n",
    "        \n",
    "        ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock1 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "        ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############\n",
    "        # 3 sequential conv2D layers: (1,40,282) --> (16, 20, 141) -> (32, 5, 35) -> (64, 1, 8)\n",
    "        self.conv2Dblock2 = nn.Sequential(\n",
    "            \n",
    "            # 1st 2D convolution layer\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, # input volume depth == input channel dim == 1\n",
    "                out_channels=16, # expand output feature map volume's depth to 16\n",
    "                kernel_size=3, # typical 3*3 stride 1 kernel\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16), # batch normalize the output feature map before activation\n",
    "            nn.ReLU(), # feature map --> activation map\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size\n",
    "            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training\n",
    "            \n",
    "            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel\n",
    "            nn.Conv2d(\n",
    "                in_channels=16, \n",
    "                out_channels=32, # expand output feature map volume's depth to 32\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters\n",
    "            nn.Dropout(p=0.3), \n",
    "            \n",
    "            # 3rd 2D convolution layer identical to last except output dim\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64, # expand output feature map volume's depth to 64\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "\n",
    "        ################# FINAL LINEAR BLOCK ####################\n",
    "        # Linear softmax layer to take final concatenated embedding tensor \n",
    "        #    from parallel 2D convolutional and transformer blocks, output 8 logits \n",
    "        # Each full convolution block outputs (64*1*8) embedding flattened to dim 512 1D array \n",
    "        # Full transformer block outputs 40*70 feature map, which we time-avg to dim 40 1D array\n",
    "        # 512*2+40 == 1064 input features --> 8 output emotions \n",
    "        self.fc1_linear = nn.Linear(512*2+40,num_emotions) \n",
    "        \n",
    "        ### Softmax layer for the 8 output logits from final FC linear layer \n",
    "        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding\n",
    "        \n",
    "    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks\n",
    "    def forward(self,x):\n",
    "        \n",
    "        ############ 1st parallel Conv2D block: 4 Convolutional layers ############################\n",
    "        # create final feature embedding from 1st convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) \n",
    "        \n",
    "        ############ 2nd parallel Conv2D block: 4 Convolutional layers #############################\n",
    "        # create final feature embedding from 2nd convolutional layer \n",
    "        # input features pased through 4 sequential 2D convolutional layers\n",
    "        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time\n",
    "        \n",
    "        # flatten final 64*1*8 feature map from convolutional layers to length 512 1D array \n",
    "        # skip the 1st (N/batch) dimension when flattening\n",
    "        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) \n",
    "        \n",
    "         \n",
    "        ########## 4-encoder-layer Transformer block w/ 40-->512-->40 feedfwd network ##############\n",
    "        # maxpool input feature map: 1*40*282 w/ 1*4 kernel --> 1*40*70\n",
    "        x_maxpool = self.transformer_maxpool(x)\n",
    "\n",
    "        # remove channel dim: 1*40*70 --> 40*70\n",
    "        x_maxpool_reduced = torch.squeeze(x_maxpool,1)\n",
    "        \n",
    "        # convert maxpooled feature map format: batch * freq * time ---> time * batch * freq format\n",
    "        # because transformer encoder layer requires tensor in format: time * batch * embedding (freq)\n",
    "        x = x_maxpool_reduced.permute(2,0,1) \n",
    "        \n",
    "        # finally, pass reduced input feature map x into transformer encoder layers\n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "        \n",
    "        # create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\n",
    "        # transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\n",
    "        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --> 40\n",
    "        \n",
    "        ############# concatenate freq embeddings from convolutional and transformer blocks ######\n",
    "        # concatenate embedding tensors output by parallel 2*conv and 1*transformer blocks\n",
    "        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  \n",
    "\n",
    "        ######### final FC linear layer, need logits for loss #########################\n",
    "        output_logits = self.fc1_linear(complete_embedding)  \n",
    "        \n",
    "        ######### Final Softmax layer: use logits from FC linear, get softmax for prediction ######\n",
    "        output_softmax = self.softmax_out(output_logits)\n",
    "        \n",
    "        # need output logits to compute cross entropy loss, need softmax probabilities to predict class\n",
    "        return output_logits, output_softmax          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/devlupin/ser/dataLoader_version.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m parallel_all_you_want(\u001b[39mlen\u001b[39m(emotions_dict))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# include input feature map dims in call to summary()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m,\u001b[39m40\u001b[39;49m,\u001b[39m282\u001b[39;49m))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/devlupin/ser/dataLoader_version.ipynb 셀 4\u001b[0m in \u001b[0;36mparallel_all_you_want.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m x \u001b[39m=\u001b[39m x_maxpool_reduced\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \n\u001b[0;32m    <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# finally, pass reduced input feature map x into transformer encoder layers\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m transformer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m# create final feature emedding from transformer layer by taking mean in the time dimension (now the 0th dim)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39m# transformer outputs 2x40 (MFCC embedding*time) feature map, take mean of columns i.e. take time average\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/Users/devlupin/ser/dataLoader_version.ipynb#W3sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m transformer_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(transformer_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m# dim 40x70 --> 40\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1126\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    199\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 202\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\module.py:1128\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1125\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1126\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1128\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    342\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    345\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    351\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 352\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    353\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    354\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    355\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torch\\nn\\modules\\module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1130\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1131\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[0;32m   1132\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torchsummary\\torchsummary.py:22\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[1;34m(module, input, output)\u001b[0m\n\u001b[0;32m     20\u001b[0m summary[m_key][\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m---> 22\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[0;32m     23\u001b[0m         [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(o\u001b[39m.\u001b[39msize())[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m output\n\u001b[0;32m     24\u001b[0m     ]\n\u001b[0;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output\u001b[39m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ser\\lib\\site-packages\\torchsummary\\torchsummary.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m summary[m_key][\u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m     22\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[1;32m---> 23\u001b[0m         [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(o\u001b[39m.\u001b[39;49msize())[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m output\n\u001b[0;32m     24\u001b[0m     ]\n\u001b[0;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "emotions_dict ={\n",
    "    '0':'surprised',\n",
    "    '1':'neutral',\n",
    "    '2':'calm',\n",
    "    '3':'happy',\n",
    "    '4':'sad',\n",
    "    '5':'angry',\n",
    "    '6':'fearful',\n",
    "    '7':'disgust'\n",
    "}\n",
    "\n",
    "# need device to instantiate model\n",
    "device = 'cuda'\n",
    "\n",
    "# instantiate model for 8 emotions and move to GPU \n",
    "model = parallel_all_you_want(len(emotions_dict)).to(device)\n",
    "\n",
    "# include input feature map dims in call to summary()\n",
    "# summary(model, input_size=(1,40,282))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(predictions, targets): \n",
    "    return nn.CrossEntropyLoss()(input=predictions, target=targets)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.001, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, criterion, optimizer):\n",
    "    \n",
    "    # define the training step of the training phase\n",
    "    def train_step(X,Y):\n",
    "        \n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X.float())\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        \n",
    "        Y = Y.type(torch.LongTensor)\n",
    "        Y = Y.to(device)\n",
    "        \n",
    "        # compute loss on logits because nn.CrossEntropyLoss implements log softmax\n",
    "        loss = criterion(output_logits, Y) \n",
    "        \n",
    "        # compute gradients for the optimizer to use \n",
    "        loss.backward()\n",
    "        \n",
    "        # update network parameters based on gradient stored (by calling loss.backward())\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero out gradients for next pass\n",
    "        # pytorch accumulates gradients from backwards passes (convenient for RNNs)\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,criterion):\n",
    "    def validate(X,Y):\n",
    "        \n",
    "        # don't want to update any network parameters on validation passes: don't need gradient\n",
    "        # wrap in torch.no_grad to save memory and compute in validation phase: \n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            # set model to validation phase i.e. turn off dropout and batchnorm layers \n",
    "            model.eval()\n",
    "      \n",
    "            # get the model's predictions on the validation set\n",
    "            output_logits, output_softmax = model(X.float())\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "\n",
    "            # calculate the mean accuracy over the entire validation set\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            \n",
    "            Y = Y.type(torch.LongTensor)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            # compute error from logits (nn.crossentropy implements softmax)\n",
    "            loss = criterion(output_logits,Y)\n",
    "            \n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.flag = True\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            self.flag = False\n",
    "            print(f'\\nEarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_save_checkpoint(): \n",
    "    def save_checkpoint(optimizer, model, epoch, filename):\n",
    "        checkpoint_dict = {\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model': model.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "    return save_checkpoint\n",
    "\n",
    "def load_checkpoint(optimizer, model, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    model.load_state_dict(checkpoint_dict['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda selected\n",
      "Number of trainable params:  248488\n"
     ]
    }
   ],
   "source": [
    "# get training set size to calculate # iterations and minibatch indices\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "# pick minibatch size (of 32... always)\n",
    "minibatch = 128\n",
    "\n",
    "# set device to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device} selected')\n",
    "\n",
    "# instantiate model and move to GPU for training\n",
    "model = parallel_all_you_want(num_emotions=len(emotions_dict)).to(device) \n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "\n",
    "# encountered bugs in google colab only, unless I explicitly defined optimizer in this cell...\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "# instantiate the checkpoint save function\n",
    "save_checkpoint = make_save_checkpoint()\n",
    "\n",
    "# instantiate the training step function \n",
    "train_step = make_train_step(model, criterion, optimizer=optimizer)\n",
    "\n",
    "# instantiate the validation loop function\n",
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "# instantiate lists to hold scalar performance metrics to plot later\n",
    "train_losses=[]\n",
    "valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WAVDataset(Dataset):\n",
    "    \"\"\"Some Information about WAV dataset\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        super(WAVDataset, self).__init__()\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "_train = WAVDataset(X_train, y_train)\n",
    "_valid = WAVDataset(X_valid, y_valid)\n",
    "_test = WAVDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'batch_size' : 128, \n",
    "          'shuffle': True, \n",
    "          'num_workers': 0} if torch.cuda.is_available() else {}\n",
    "\n",
    "train_loader = DataLoader(_train, **kwargs)\n",
    "val_loader = DataLoader(_valid, **kwargs)\n",
    "test_loader = DataLoader(_test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def __train(optimizer, model, num_epochs, train, val):\n",
    "    early_stopping = EarlyStopping(patience=30)\n",
    "    \n",
    "    checkpoint_filename = './models/checkpoints/parallel_all_you_want.pkl'\n",
    "    \n",
    "    minibatch = 128\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc='training......'):\n",
    "        model.train()\n",
    "        \n",
    "        epoch_acc = 0 \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        i = 0\n",
    "        for data, label in train:\n",
    "            batch_start = i * minibatch \n",
    "            # ensure we don't go out of the bounds of our training set:\n",
    "            batch_end = min(batch_start + minibatch, train_size) \n",
    "            # ensure we don't have an index error\n",
    "            actual_batch_size = batch_end-batch_start \n",
    "            \n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            \n",
    "            loss, acc = train_step(data, label) \n",
    "            \n",
    "            epoch_acc += acc * actual_batch_size / train_size\n",
    "            epoch_loss += loss * actual_batch_size / train_size\n",
    "            \n",
    "            print('\\r'+f'Epoch {epoch}: iteration {i}/{len(train)}',end='')\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            valid_loss, valid_acc, _ = validate(data, label)\n",
    "            break\n",
    "            \n",
    "        train_losses.append(epoch_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        early_stopping(valid_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\n\\n[*] Early Stop - {epoch} epochs\")\n",
    "            print(f'[*] Best validation loss - {valid_loss}')\n",
    "            break\n",
    "                  \n",
    "        if early_stopping.flag:\n",
    "            # Save checkpoint of the model\n",
    "            save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
    "        \n",
    "        # keep track of each epoch's progress\n",
    "        print(f'\\nEpoch {epoch} --- loss:{epoch_loss:.3f}, Epoch accuracy:{epoch_acc:.2f}%, Validation loss:{valid_loss:.3f}, Validation accuracy:{valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75381aa5acb94b7cbf1358c768f30f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training......:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: iteration 26/27\n",
      "Epoch 0 --- loss:5.384, Epoch accuracy:17.61%, Validation loss:1.992, Validation accuracy:25.00%\n",
      "Epoch 1: iteration 26/27\n",
      "Epoch 1 --- loss:1.828, Epoch accuracy:30.31%, Validation loss:1.913, Validation accuracy:24.22%\n",
      "Epoch 2: iteration 26/27\n",
      "Epoch 2 --- loss:1.665, Epoch accuracy:36.04%, Validation loss:1.821, Validation accuracy:30.47%\n",
      "Epoch 3: iteration 26/27\n",
      "Epoch 3 --- loss:1.585, Epoch accuracy:39.44%, Validation loss:1.764, Validation accuracy:31.25%\n",
      "Epoch 4: iteration 26/27\n",
      "Epoch 4 --- loss:1.547, Epoch accuracy:41.94%, Validation loss:1.587, Validation accuracy:37.50%\n",
      "Epoch 5: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 5 --- loss:1.500, Epoch accuracy:43.48%, Validation loss:1.596, Validation accuracy:39.84%\n",
      "Epoch 6: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 6 --- loss:1.451, Epoch accuracy:44.41%, Validation loss:1.606, Validation accuracy:42.19%\n",
      "Epoch 7: iteration 26/27\n",
      "EarlyStopping counter: 3 out of 30\n",
      "\n",
      "Epoch 7 --- loss:1.401, Epoch accuracy:47.86%, Validation loss:1.605, Validation accuracy:39.06%\n",
      "Epoch 8: iteration 26/27\n",
      "Epoch 8 --- loss:1.381, Epoch accuracy:47.75%, Validation loss:1.381, Validation accuracy:46.09%\n",
      "Epoch 9: iteration 26/27\n",
      "Epoch 9 --- loss:1.348, Epoch accuracy:49.69%, Validation loss:1.344, Validation accuracy:57.81%\n",
      "Epoch 10: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 10 --- loss:1.344, Epoch accuracy:49.38%, Validation loss:1.468, Validation accuracy:48.44%\n",
      "Epoch 11: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 11 --- loss:1.303, Epoch accuracy:50.07%, Validation loss:1.412, Validation accuracy:47.66%\n",
      "Epoch 12: iteration 26/27\n",
      "Epoch 12 --- loss:1.254, Epoch accuracy:53.24%, Validation loss:1.312, Validation accuracy:53.91%\n",
      "Epoch 13: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 13 --- loss:1.232, Epoch accuracy:54.08%, Validation loss:1.509, Validation accuracy:43.75%\n",
      "Epoch 14: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 14 --- loss:1.198, Epoch accuracy:55.65%, Validation loss:1.345, Validation accuracy:55.47%\n",
      "Epoch 15: iteration 26/27\n",
      "Epoch 15 --- loss:1.167, Epoch accuracy:56.06%, Validation loss:1.271, Validation accuracy:51.56%\n",
      "Epoch 16: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 16 --- loss:1.177, Epoch accuracy:55.04%, Validation loss:1.286, Validation accuracy:57.03%\n",
      "Epoch 17: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 17 --- loss:1.182, Epoch accuracy:55.16%, Validation loss:1.522, Validation accuracy:47.66%\n",
      "Epoch 18: iteration 26/27\n",
      "EarlyStopping counter: 3 out of 30\n",
      "\n",
      "Epoch 18 --- loss:1.123, Epoch accuracy:59.20%, Validation loss:1.362, Validation accuracy:49.22%\n",
      "Epoch 19: iteration 26/27\n",
      "EarlyStopping counter: 4 out of 30\n",
      "\n",
      "Epoch 19 --- loss:1.090, Epoch accuracy:58.82%, Validation loss:1.277, Validation accuracy:51.56%\n",
      "Epoch 20: iteration 26/27\n",
      "Epoch 20 --- loss:1.071, Epoch accuracy:59.72%, Validation loss:1.195, Validation accuracy:58.59%\n",
      "Epoch 21: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 21 --- loss:1.049, Epoch accuracy:61.12%, Validation loss:1.344, Validation accuracy:50.78%\n",
      "Epoch 22: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 22 --- loss:1.055, Epoch accuracy:60.04%, Validation loss:1.291, Validation accuracy:51.56%\n",
      "Epoch 23: iteration 26/27\n",
      "EarlyStopping counter: 3 out of 30\n",
      "\n",
      "Epoch 23 --- loss:1.002, Epoch accuracy:63.15%, Validation loss:1.383, Validation accuracy:52.34%\n",
      "Epoch 24: iteration 26/27\n",
      "EarlyStopping counter: 4 out of 30\n",
      "\n",
      "Epoch 24 --- loss:0.994, Epoch accuracy:62.37%, Validation loss:1.228, Validation accuracy:56.25%\n",
      "Epoch 25: iteration 26/27\n",
      "EarlyStopping counter: 5 out of 30\n",
      "\n",
      "Epoch 25 --- loss:0.965, Epoch accuracy:64.34%, Validation loss:1.346, Validation accuracy:52.34%\n",
      "Epoch 26: iteration 26/27\n",
      "Epoch 26 --- loss:0.954, Epoch accuracy:65.71%, Validation loss:1.183, Validation accuracy:58.59%\n",
      "Epoch 27: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 27 --- loss:0.963, Epoch accuracy:64.20%, Validation loss:1.378, Validation accuracy:49.22%\n",
      "Epoch 28: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 28 --- loss:0.915, Epoch accuracy:66.11%, Validation loss:1.388, Validation accuracy:55.47%\n",
      "Epoch 29: iteration 26/27\n",
      "Epoch 29 --- loss:0.905, Epoch accuracy:66.32%, Validation loss:1.128, Validation accuracy:60.94%\n",
      "Epoch 30: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 30 --- loss:0.943, Epoch accuracy:65.27%, Validation loss:1.223, Validation accuracy:56.25%\n",
      "Epoch 31: iteration 26/27\n",
      "Epoch 31 --- loss:0.847, Epoch accuracy:69.08%, Validation loss:1.126, Validation accuracy:57.03%\n",
      "Epoch 32: iteration 26/27\n",
      "EarlyStopping counter: 1 out of 30\n",
      "\n",
      "Epoch 32 --- loss:0.821, Epoch accuracy:68.79%, Validation loss:1.428, Validation accuracy:51.56%\n",
      "Epoch 33: iteration 26/27\n",
      "EarlyStopping counter: 2 out of 30\n",
      "\n",
      "Epoch 33 --- loss:0.813, Epoch accuracy:70.24%, Validation loss:1.240, Validation accuracy:54.69%\n",
      "Epoch 34: iteration 26/27\n",
      "EarlyStopping counter: 3 out of 30\n",
      "\n",
      "Epoch 34 --- loss:0.787, Epoch accuracy:71.14%, Validation loss:1.404, Validation accuracy:55.47%\n",
      "Epoch 35: iteration 26/27\n",
      "EarlyStopping counter: 4 out of 30\n",
      "\n",
      "Epoch 35 --- loss:0.805, Epoch accuracy:69.40%, Validation loss:1.265, Validation accuracy:56.25%\n",
      "Epoch 36: iteration 26/27\n",
      "EarlyStopping counter: 5 out of 30\n",
      "\n",
      "Epoch 36 --- loss:0.755, Epoch accuracy:72.25%, Validation loss:1.200, Validation accuracy:57.03%\n",
      "Epoch 37: iteration 26/27\n",
      "EarlyStopping counter: 6 out of 30\n",
      "\n",
      "Epoch 37 --- loss:0.739, Epoch accuracy:72.45%, Validation loss:1.227, Validation accuracy:54.69%\n",
      "Epoch 38: iteration 26/27\n",
      "EarlyStopping counter: 7 out of 30\n",
      "\n",
      "Epoch 38 --- loss:0.737, Epoch accuracy:73.29%, Validation loss:1.178, Validation accuracy:61.72%\n",
      "Epoch 39: iteration 26/27\n",
      "EarlyStopping counter: 8 out of 30\n",
      "\n",
      "Epoch 39 --- loss:0.694, Epoch accuracy:74.37%, Validation loss:1.173, Validation accuracy:57.81%\n",
      "Epoch 40: iteration 26/27\n",
      "EarlyStopping counter: 9 out of 30\n",
      "\n",
      "Epoch 40 --- loss:0.699, Epoch accuracy:74.08%, Validation loss:1.420, Validation accuracy:55.47%\n",
      "Epoch 41: iteration 26/27\n",
      "EarlyStopping counter: 10 out of 30\n",
      "\n",
      "Epoch 41 --- loss:0.654, Epoch accuracy:75.68%, Validation loss:1.348, Validation accuracy:57.81%\n",
      "Epoch 42: iteration 26/27\n",
      "EarlyStopping counter: 11 out of 30\n",
      "\n",
      "Epoch 42 --- loss:0.663, Epoch accuracy:75.41%, Validation loss:1.377, Validation accuracy:58.59%\n",
      "Epoch 43: iteration 26/27\n",
      "EarlyStopping counter: 12 out of 30\n",
      "\n",
      "Epoch 43 --- loss:0.648, Epoch accuracy:76.26%, Validation loss:1.171, Validation accuracy:60.16%\n",
      "Epoch 44: iteration 26/27\n",
      "EarlyStopping counter: 13 out of 30\n",
      "\n",
      "Epoch 44 --- loss:0.610, Epoch accuracy:78.12%, Validation loss:1.422, Validation accuracy:53.12%\n",
      "Epoch 45: iteration 26/27\n",
      "EarlyStopping counter: 14 out of 30\n",
      "\n",
      "Epoch 45 --- loss:0.618, Epoch accuracy:76.93%, Validation loss:1.343, Validation accuracy:55.47%\n",
      "Epoch 46: iteration 26/27\n",
      "EarlyStopping counter: 15 out of 30\n",
      "\n",
      "Epoch 46 --- loss:0.583, Epoch accuracy:78.67%, Validation loss:1.477, Validation accuracy:55.47%\n",
      "Epoch 47: iteration 26/27\n",
      "EarlyStopping counter: 16 out of 30\n",
      "\n",
      "Epoch 47 --- loss:0.562, Epoch accuracy:79.69%, Validation loss:1.474, Validation accuracy:56.25%\n",
      "Epoch 48: iteration 26/27\n",
      "EarlyStopping counter: 17 out of 30\n",
      "\n",
      "Epoch 48 --- loss:0.560, Epoch accuracy:79.98%, Validation loss:1.375, Validation accuracy:60.94%\n",
      "Epoch 49: iteration 26/27\n",
      "EarlyStopping counter: 18 out of 30\n",
      "\n",
      "Epoch 49 --- loss:0.532, Epoch accuracy:81.46%, Validation loss:1.411, Validation accuracy:57.03%\n",
      "Epoch 50: iteration 26/27\n",
      "EarlyStopping counter: 19 out of 30\n",
      "\n",
      "Epoch 50 --- loss:0.523, Epoch accuracy:81.14%, Validation loss:1.293, Validation accuracy:59.38%\n",
      "Epoch 51: iteration 26/27\n",
      "EarlyStopping counter: 20 out of 30\n",
      "\n",
      "Epoch 51 --- loss:0.497, Epoch accuracy:81.92%, Validation loss:1.278, Validation accuracy:64.84%\n",
      "Epoch 52: iteration 26/27\n",
      "EarlyStopping counter: 21 out of 30\n",
      "\n",
      "Epoch 52 --- loss:0.513, Epoch accuracy:82.13%, Validation loss:1.455, Validation accuracy:57.03%\n",
      "Epoch 53: iteration 26/27\n",
      "EarlyStopping counter: 22 out of 30\n",
      "\n",
      "Epoch 53 --- loss:0.499, Epoch accuracy:81.69%, Validation loss:1.247, Validation accuracy:62.50%\n",
      "Epoch 54: iteration 26/27\n",
      "EarlyStopping counter: 23 out of 30\n",
      "\n",
      "Epoch 54 --- loss:0.493, Epoch accuracy:82.21%, Validation loss:1.284, Validation accuracy:64.06%\n",
      "Epoch 55: iteration 26/27\n",
      "EarlyStopping counter: 24 out of 30\n",
      "\n",
      "Epoch 55 --- loss:0.454, Epoch accuracy:83.58%, Validation loss:1.447, Validation accuracy:62.50%\n",
      "Epoch 56: iteration 26/27\n",
      "EarlyStopping counter: 25 out of 30\n",
      "\n",
      "Epoch 56 --- loss:0.435, Epoch accuracy:84.74%, Validation loss:1.636, Validation accuracy:57.03%\n",
      "Epoch 57: iteration 26/27\n",
      "EarlyStopping counter: 26 out of 30\n",
      "\n",
      "Epoch 57 --- loss:0.453, Epoch accuracy:83.84%, Validation loss:1.220, Validation accuracy:59.38%\n",
      "Epoch 58: iteration 26/27\n",
      "EarlyStopping counter: 27 out of 30\n",
      "\n",
      "Epoch 58 --- loss:0.421, Epoch accuracy:84.77%, Validation loss:1.363, Validation accuracy:63.28%\n",
      "Epoch 59: iteration 26/27\n",
      "EarlyStopping counter: 28 out of 30\n",
      "\n",
      "Epoch 59 --- loss:0.432, Epoch accuracy:83.99%, Validation loss:1.333, Validation accuracy:64.84%\n",
      "Epoch 60: iteration 26/27\n",
      "EarlyStopping counter: 29 out of 30\n",
      "\n",
      "Epoch 60 --- loss:0.422, Epoch accuracy:84.57%, Validation loss:1.338, Validation accuracy:62.50%\n",
      "Epoch 61: iteration 26/27\n",
      "EarlyStopping counter: 30 out of 30\n",
      "\n",
      "\n",
      "[*] Early Stop - 61 epochs\n",
      "[*] Best validation loss - 1.5591670274734497\n"
     ]
    }
   ],
   "source": [
    "# choose number of epochs higher than reasonable so we can manually stop training \n",
    "num_epochs = 500\n",
    "\n",
    "# train it!\n",
    "__train(optimizer, model, num_epochs, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93268fa09282bea7dc8c8b8652c326b24cb6e292d11803ad71001fa0b34f3e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
